{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS Snow Cover Data Processing with Dask and Azure Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odc.stac version: 0.4.0\n",
      "xarray version: 2025.4.0\n",
      "zarr version: 2.18.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import easysnowdata\n",
    "import modis_masking\n",
    "import coiled\n",
    "import tqdm\n",
    "import logging\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import adlfs\n",
    "import pathlib\n",
    "from dask.distributed import as_completed\n",
    "#odc.stac.configure_rio(cloud_defaults=True)\n",
    "\n",
    "import odc.stac\n",
    "print(f'odc.stac version: {odc.stac.__version__}')\n",
    "print(f'xarray version: {xr.__version__}')\n",
    "print(f'zarr version: {zarr.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WY_start = 2015\n",
    "WY_end = 2024\n",
    "\n",
    "# get token from https://github.com/egagli/azure_authentication/raw/main/sas_token.txt\n",
    "sas_token = pathlib.Path(\"sas_token.txt\").read_text()\n",
    "\n",
    "store = adlfs.AzureBlobFileSystem(\n",
    "    account_name=\"snowmelt\", credential=sas_token\n",
    ").get_mapper(\"snowmelt/snow_cover/global_modis_snow_cover_4.zarr\") # make sure this matches the store created in make_grid_and_zarr_store.ipynb\n",
    "\n",
    "# created in make_grid_and_zarr_store.ipynb\n",
    "with open('modis_tile_processing_list.txt', 'r') as f:\n",
    "    tile_processing_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define processing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note to self as leaving 6/13....... we started new, now we just have to run this processing! do it just for polar tiles, then view, then also view MAD. if looks good, run everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(tile, store):\n",
    "\n",
    "    h, v = (int(part[1:]) for part in tile.split('_'))\n",
    "\n",
    "    # odc.stac.configure_rio(cloud_defaults=True)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger('azure').setLevel(logging.WARNING)\n",
    "\n",
    "    logger.info(f\"Starting process for tile {tile}\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        # logger.info(f\"Zarr store opened successfully\")\n",
    "\n",
    "        hemisphere = \"northern\" if v < 9 else \"southern\"\n",
    "\n",
    "        # logger.info(f\"Fetching MODIS data for tile {tile}\")\n",
    "        if hemisphere == \"northern\":\n",
    "            modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "                vertical_tile=v,\n",
    "                horizontal_tile=h,\n",
    "                start_date=f\"{WY_start-2}-10-01\", # normally should be WY_start-1, but we want to get data from WY before, as October 1st acquistion in NH already has fill values\n",
    "                end_date=f\"{WY_end}-09-30\",\n",
    "                #chunks={},\n",
    "                #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "                chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "            ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "        else:\n",
    "            modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "                vertical_tile=v,\n",
    "                horizontal_tile=h,\n",
    "                start_date=f\"{WY_start-1}-04-01\", # normally should be WY_start, but we want to get data from WY before, as October 1st acquistion in NH already has fill values\n",
    "                end_date=f\"{WY_end+1}-03-31\",\n",
    "                #chunks={},\n",
    "                #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "                chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "            ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "        # logger.info(f\"Processing MODIS data for tile {tile}\")\n",
    "        modis_snow_da.coords[\"water_year\"] = (\n",
    "            \"time\",\n",
    "            pd.to_datetime(modis_snow_da.time).map(\n",
    "                lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere)\n",
    "            ),\n",
    "        )\n",
    "        modis_snow_da.coords[\"DOWY\"] = (\n",
    "            \"time\",\n",
    "            pd.to_datetime(modis_snow_da.time).map(\n",
    "                lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if (v>=15) | (v<=2):# if north or south pole, remove strange no snow artifacts around time of darkness (check to see if darkness in the scene?)\n",
    "            # if a scene contains significant no decision (1) / night (11) values, change no snow (25) to fill (255)?\n",
    "\n",
    "            value25_da = modis_snow_da.where(lambda x: x == 25).count(dim=[\"x\",\"y\"])\n",
    "            value200_da = modis_snow_da.where(lambda x: x == 200).count(dim=[\"x\",\"y\"])\n",
    "            no_decision_and_night_counts = modis_snow_da.where(lambda x: (x == 1) | (x == 11)).count(dim=[\"x\",\"y\"])\n",
    "\n",
    "            land_area_da = value200_da + value25_da\n",
    "            max_land_pixels = land_area_da.max(dim='time')\n",
    "            bad_pixel_thresh = int(0.05*int(max_land_pixels))\n",
    "\n",
    "            scenes_with_polar_night = no_decision_and_night_counts > bad_pixel_thresh\n",
    "            scenes_with_polar_night_buffered = (scenes_with_polar_night.shift(time=-1).fillna(0) | scenes_with_polar_night | scenes_with_polar_night.shift(time=1).fillna(0)).astype(int)\n",
    "            backward_check = scenes_with_polar_night_buffered.rolling(time=4, center=False).sum() >= 4  # forward-looking\n",
    "            forward_check = scenes_with_polar_night_buffered[::-1].rolling(time=4, center=False).sum()[::-1] >= 4 # backward-looking\n",
    "            center_check = scenes_with_polar_night_buffered.rolling(time=4, center=True).sum() >= 4  # center-looking\n",
    "\n",
    "            # A position should be kept if it's part of a sequence of 4+ when looking in any direction\n",
    "            scenes_with_polar_night_buffered_filtered = scenes_with_polar_night_buffered.where(\n",
    "                backward_check | forward_check | center_check,\n",
    "                other=0\n",
    "            ).astype(bool).chunk(dict(time=-1))\n",
    "\n",
    "            scenes_with_polar_night_buffered_filtered_complete = (\n",
    "                scenes_with_polar_night_buffered_filtered.where(lambda x: x == 1)\n",
    "                .interpolate_na(\n",
    "                    dim=\"time\", method=\"nearest\", max_gap=pd.Timedelta(days=80)\n",
    "                )\n",
    "                .where(lambda x: x == 1, other=0)\n",
    "                .astype(bool)\n",
    "            )\n",
    "\n",
    "            modis_snow_da = modis_snow_da.where(\n",
    "                ~((modis_snow_da == 25) & (scenes_with_polar_night_buffered_filtered_complete)), other=255)\n",
    "\n",
    "        # logger.info(f\"Applying binarize_with_cloud_filling for tile {tile}\")\n",
    "        effective_snow_da = modis_masking.binarize_with_cloud_filling(modis_snow_da)\n",
    "\n",
    "        effective_snow_complete_wys_da = modis_masking.align_wy_start(effective_snow_da, hemisphere=hemisphere)\n",
    "\n",
    "        # logger.info(f\"Calculating seasonal snow presence for tile {tile}\")\n",
    "        # seasonal_snow_presence = effective_snow_da.groupby(\"water_year\").apply(\n",
    "        #     modis_masking.get_max_consec_snow_days_SAD_SDD_one_WY\n",
    "        # )\n",
    "        seasonal_snow_cover_ds = effective_snow_complete_wys_da.groupby('water_year').apply(modis_masking.get_max_consec_snow_days_SAD_SDD_one_WY).compute()\n",
    "\n",
    "        seasonal_snow_cover_ds = seasonal_snow_cover_ds.sel(water_year=slice(WY_start, WY_end))\n",
    "        # logger.info(f\"Writing results to zarr store for tile {tile}\")\n",
    "\n",
    "        num_years = len(seasonal_snow_cover_ds.water_year)\n",
    "        y_slice = slice(v * 2400, (v + 1) * 2400)\n",
    "        x_slice = slice(h * 2400, (h + 1) * 2400)\n",
    "\n",
    "        existing_ds = xr.open_zarr(store, consolidated=True)\n",
    "        y_coords = existing_ds.y[y_slice].values\n",
    "        x_coords = existing_ds.x[x_slice].values\n",
    "\n",
    "        if np.allclose(y_coords, seasonal_snow_cover_ds.y.values, atol=0.1) or np.allclose(x_coords, seasonal_snow_cover_ds.x.values, atol=0.1):\n",
    "            seasonal_snow_cover_ds = seasonal_snow_cover_ds.assign_coords(y=y_coords, x=x_coords)\n",
    "        else:\n",
    "            logger.error(f\"y or x coordinates do not match for tile {tile}\")\n",
    "            raise ValueError(f\"y or x coordinates do not match for tile {tile}\")\n",
    "\n",
    "        # remove _FillValue from coords\n",
    "        for var in seasonal_snow_cover_ds.data_vars:\n",
    "            seasonal_snow_cover_ds[var] = seasonal_snow_cover_ds[var].drop_attrs()\n",
    "\n",
    "        seasonal_snow_cover_ds.drop_vars(\"spatial_ref\").chunk(\n",
    "            {\"water_year\": 1, \"y\": 2400, \"x\": 2400}\n",
    "        ).to_zarr(store, region=\"auto\", mode=\"r+\", consolidated=True)\n",
    "        # logger.info(f\"Tile {tile} processed and written successfully\")\n",
    "        # existing_ds.attrs['processed_tiles'].append(tile)\n",
    "        # logger.info(f\"Tile {tile} processed and written, added to processed_tiles list\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"(PT) Error processing tile {tile}: {str(e)}\")\n",
    "        logger.error(f\"(PT) Traceback: {traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Dask Cluster with Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d2849cdb6d4f8fb174088aefc6a1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭──────────────────────────────── <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Package Info</span> ────────────────────────────────╮\n",
       "│                                       ╷                                      │\n",
       "│  <span style=\"font-weight: bold\"> Package                             </span>│<span style=\"font-weight: bold\"> Note                               </span>  │\n",
       "│ ╶─────────────────────────────────────┼────────────────────────────────────╴ │\n",
       "│   coiled_local_MODIS_seasonal_snow_m… │ Source wheel built from              │\n",
       "│                                       │ ~/repos/MODIS_seasonal_snow_mask     │\n",
       "│   global_snowmelt_runoff_onset        │ Wheel built from                     │\n",
       "│                                       │ ~/repos/global_snowmelt_runoff_ons   │\n",
       "│                                       │ et                                   │\n",
       "│                                       ╵                                      │\n",
       "╰──────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭──────────────────────────────── \u001b[1;32mPackage Info\u001b[0m ────────────────────────────────╮\n",
       "│                                       ╷                                      │\n",
       "│  \u001b[1m \u001b[0m\u001b[1mPackage                            \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mNote                              \u001b[0m\u001b[1m \u001b[0m  │\n",
       "│ ╶─────────────────────────────────────┼────────────────────────────────────╴ │\n",
       "│   coiled_local_MODIS_seasonal_snow_m… │ Source wheel built from              │\n",
       "│                                       │ ~/repos/MODIS_seasonal_snow_mask     │\n",
       "│   global_snowmelt_runoff_onset        │ Wheel built from                     │\n",
       "│                                       │ ~/repos/global_snowmelt_runoff_ons   │\n",
       "│                                       │ et                                   │\n",
       "│                                       ╵                                      │\n",
       "╰──────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b38010c630496d83800b19a8a1e708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 1363, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/ssl.py\", line 1372, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1028)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/asyncio/events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/platform/asyncio.py\", line 208, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "    ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 1417, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 1372, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "    ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "asyncio.exceptions.CancelledError\n",
      "Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 1363, in _do_ssl_handshake\n",
      "    self.socket.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/ssl.py\", line 1372, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1028)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/asyncio/events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/platform/asyncio.py\", line 208, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 691, in _handle_events\n",
      "    self._handle_read()\n",
      "    ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 1417, in _handle_read\n",
      "    self._do_ssl_handshake()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 1372, in _do_ssl_handshake\n",
      "    return self.close(exc_info=err)\n",
      "           ~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 606, in close\n",
      "    self._signal_closed()\n",
      "    ~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/tornado/iostream.py\", line 636, in _signal_closed\n",
      "    self._ssl_connect_future.exception()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "asyncio.exceptions.CancelledError\n",
      "2025-06-14 10:08:00,888 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n",
      "/home/eric/miniconda3/envs/new_global_snowmelt_runoff_onset/lib/python3.13/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "#                         n_workers=10, #30\n",
    "#                         worker_memory=\"16 GB\", #32 \n",
    "#                         worker_cpu=4, # 4,\n",
    "#                         #worker_options={\"nthreads\": 1},\n",
    "#                         #scheduler_memory=\"32 GB\",\n",
    "#                         spot_policy=\"spot\",\n",
    "#                         environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "#                         workspace=\"uwtacolab\",\n",
    "#                         )\n",
    "\n",
    "cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "                        n_workers=30, #30\n",
    "                        worker_memory=\"32 GB\", #32 \n",
    "                        worker_cpu=4, # 4,\n",
    "                        #worker_options={\"nthreads\": 1},\n",
    "                        #scheduler_memory=\"32 GB\",\n",
    "                        spot_policy=\"spot\",\n",
    "                        environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "                        workspace=\"uwtacolab\",\n",
    "                        )\n",
    "\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process MODIS Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = \"h11_v2\"\n",
    "client.submit(process_tile, tile, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_processing_list = tile_processing_list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 unprocessed tiles\n",
      "\n",
      "=== Processing batch 1/1 (3 tiles) ===\n",
      "Submitted 3 tiles for batch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1 progress:  33%|███▎      | 1/3 [02:42<05:25, 162.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile h23_v4 SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1 progress: 100%|██████████| 3/3 [02:43<00:00, 54.46s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tile h27_v5 SUCCESS\n",
      "Tile h10_v3 SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated zarr store with 3 tiles\n",
      "Batch 1 completed. Failed so far: 0\n",
      "Client restarted after batch 1\n",
      "\n",
      "=== FINAL RESULTS ===\n",
      "Total tiles processed: 3\n",
      "Failed tiles: 0\n",
      "Now consolidating metadata...\n",
      "All tiles processed successfully!!!\n"
     ]
    }
   ],
   "source": [
    "failed_tiles = []\n",
    "completed_tiles_batch = []\n",
    "BATCH_SIZE = 30  # For zarr store updates AND submission batches\n",
    "\n",
    "# Get initially processed tiles\n",
    "processed_tiles_list_initial = zarr.open(store).attrs['processed_tiles']\n",
    "\n",
    "# Filter out already processed tiles\n",
    "unprocessed_tiles = [tile for tile in tile_processing_list \n",
    "                    if tile not in processed_tiles_list_initial]\n",
    "\n",
    "print(f\"Found {len(unprocessed_tiles)} unprocessed tiles\")\n",
    "\n",
    "# Process tiles in batches of 20\n",
    "for i in range(0, len(unprocessed_tiles), BATCH_SIZE):\n",
    "    batch = unprocessed_tiles[i:i+BATCH_SIZE]\n",
    "    batch_num = i//BATCH_SIZE + 1\n",
    "    total_batches = (len(unprocessed_tiles)-1)//BATCH_SIZE + 1\n",
    "    \n",
    "    print(f\"\\n=== Processing batch {batch_num}/{total_batches} ({len(batch)} tiles) ===\")\n",
    "    \n",
    "    # Submit this batch\n",
    "    batch_futures = {}\n",
    "    for tile in batch:\n",
    "        future = client.submit(process_tile, tile, store)\n",
    "        batch_futures[future] = tile\n",
    "    \n",
    "    print(f\"Submitted {len(batch_futures)} tiles for batch {batch_num}\")\n",
    "    \n",
    "    # Process this batch to completion\n",
    "    for future in tqdm.tqdm(as_completed(batch_futures), total=len(batch_futures), \n",
    "                           desc=f\"Batch {batch_num} progress\"):\n",
    "        tile = batch_futures[future]\n",
    "        \n",
    "        try:\n",
    "            result = future.result()\n",
    "            \n",
    "            if result == True:\n",
    "                completed_tiles_batch.append(tile)\n",
    "                print(f\"Tile {tile} SUCCESS\")\n",
    "            else:\n",
    "                print(f\"Tile {tile} FAIL, adding to failed list\")\n",
    "                failed_tiles.append(tile)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Tile {tile} FAILED with exception: {str(e)}\")\n",
    "            failed_tiles.append(tile)\n",
    "    \n",
    "    # Update zarr store with all completed tiles from this batch\n",
    "    if completed_tiles_batch:\n",
    "        with zarr.open(store) as zarr_store:\n",
    "            processed_tile_list = zarr_store.attrs['processed_tiles']\n",
    "            processed_tile_list.extend(completed_tiles_batch)\n",
    "            zarr_store.attrs['processed_tiles'] = processed_tile_list\n",
    "        print(f\"Updated zarr store with {len(completed_tiles_batch)} tiles\")\n",
    "        completed_tiles_batch = []\n",
    "    \n",
    "    print(f\"Batch {batch_num} completed. Failed so far: {len(failed_tiles)}\")\n",
    "    \n",
    "    # Restart client after each batch\n",
    "    client.restart(wait_for_workers=True)\n",
    "    print(f\"Client restarted after batch {batch_num}\")\n",
    "\n",
    "# Final status report\n",
    "print(f\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Total tiles processed: {len(unprocessed_tiles) - len(failed_tiles)}\")\n",
    "print(f\"Failed tiles: {len(failed_tiles)}\")\n",
    "\n",
    "if failed_tiles:\n",
    "    print(\"Run this cell again. The following tiles could not be processed:\")\n",
    "    for tile in failed_tiles:\n",
    "        print(tile)\n",
    "else:\n",
    "    print(\"Now consolidating metadata...\")\n",
    "    zarr.consolidate_metadata(store)\n",
    "    print(\"All tiles processed successfully!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test longest consec stretch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive test of function equivalence with more and longer cases\n",
    "def comprehensive_test_vectorized_function():\n",
    "    import numpy as np\n",
    "    from numba import jit\n",
    "    \n",
    "    # Define both functions here for testing\n",
    "    def get_longest_consec_stretch_original(arr):\n",
    "        max_len = 0\n",
    "        max_start = 0\n",
    "        max_end = 0\n",
    "        current_start = None\n",
    "        \n",
    "        for i, val in enumerate(arr):\n",
    "            if val:\n",
    "                if current_start is None:\n",
    "                    current_start = i\n",
    "            else:\n",
    "                if current_start is not None:\n",
    "                    length = i - current_start\n",
    "                    if length >= max_len:\n",
    "                        max_len = length\n",
    "                        max_start = current_start\n",
    "                        max_end = i\n",
    "                    current_start = None\n",
    "        \n",
    "        if current_start is not None:\n",
    "            length = len(arr) - current_start\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                max_start = current_start\n",
    "                max_end = len(arr)\n",
    "        \n",
    "        if max_len == 0:\n",
    "            return -32768, -32768, -32768  # fill_value\n",
    "        \n",
    "        return max_start, max_end, max_len\n",
    "    \n",
    "    @jit(nopython=True)\n",
    "    def get_longest_consec_stretch_vectorized(arr):\n",
    "        n = len(arr)\n",
    "        if n == 0:\n",
    "            return -32768, -32768, -32768\n",
    "        \n",
    "        max_len = 0\n",
    "        max_start = 0\n",
    "        max_end = 0\n",
    "        current_start = -1\n",
    "        \n",
    "        for i in range(n):\n",
    "            if arr[i]:\n",
    "                if current_start == -1:\n",
    "                    current_start = i\n",
    "            else:\n",
    "                if current_start != -1:\n",
    "                    length = i - current_start\n",
    "                    if length >= max_len:\n",
    "                        max_len = length\n",
    "                        max_start = current_start\n",
    "                        max_end = i\n",
    "                    current_start = -1\n",
    "        \n",
    "        if current_start != -1:\n",
    "            length = n - current_start\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                max_start = current_start\n",
    "                max_end = n\n",
    "        \n",
    "        if max_len == 0:\n",
    "            return -32768, -32768, -32768\n",
    "        \n",
    "        return max_start, max_end, max_len\n",
    "    \n",
    "    # Comprehensive test cases\n",
    "    test_cases = [\n",
    "        # Basic cases\n",
    "        [],\n",
    "        [False],\n",
    "        [True],\n",
    "        [False, False, False],\n",
    "        [True, True, True],\n",
    "        \n",
    "        # Simple patterns\n",
    "        [True, True, False, True, True, True],\n",
    "        [False, True, True, False, False, True],\n",
    "        [True, False, True, False, True],\n",
    "        \n",
    "        # Edge cases - snow at boundaries\n",
    "        [True, False, False, False],  # Snow at start only\n",
    "        [False, False, False, True],  # Snow at end only\n",
    "        [True, True, False, False],   # Snow at start, none at end\n",
    "        [False, False, True, True],   # No snow at start, snow at end\n",
    "        \n",
    "        # Full coverage cases\n",
    "        [True] * 10,    # All snow\n",
    "        [False] * 10,   # No snow\n",
    "        \n",
    "        # Realistic MODIS water year patterns (46 time steps)\n",
    "        # Early season snow pattern\n",
    "        [False] * 5 + [True] * 15 + [False] * 26,\n",
    "        \n",
    "        # Mid-season with gaps\n",
    "        [False] * 8 + [True] * 12 + [False] * 6 + [True] * 10 + [False] * 10,\n",
    "        \n",
    "        # Late season pattern\n",
    "        [True] * 20 + [False] * 26,\n",
    "        \n",
    "        # Full season snow\n",
    "        [True] * 46,\n",
    "        \n",
    "        # No snow season\n",
    "        [False] * 46,\n",
    "        \n",
    "        # Complex multi-period pattern\n",
    "        [False] * 3 + [True] * 8 + [False] * 5 + [True] * 15 + [False] * 7 + [True] * 8,\n",
    "        \n",
    "        # Very long arrays (simulate multiple years)\n",
    "        # Long early season\n",
    "        [False] * 20 + [True] * 50 + [False] * 30,\n",
    "        \n",
    "        # Multiple long periods\n",
    "        [True] * 25 + [False] * 10 + [True] * 35 + [False] * 15 + [True] * 15,\n",
    "        \n",
    "        # Intermittent pattern (cloud gaps)\n",
    "        ([True, False, True, True, False, False, True, True, True, False] * 10),\n",
    "        \n",
    "        # Very large array with complex pattern\n",
    "        ([False] * 10 + [True] * 20 + [False] * 5 + [True] * 30 + \n",
    "         [False] * 8 + [True] * 15 + [False] * 12 + [True] * 25 + [False] * 20),\n",
    "        \n",
    "        # Alternating single values (worst case for algorithm)\n",
    "        [True, False] * 50,\n",
    "        [False, True] * 50,\n",
    "        \n",
    "        # Random patterns of various lengths\n",
    "        np.random.choice([True, False], size=20, p=[0.4, 0.6]).tolist(),\n",
    "        np.random.choice([True, False], size=50, p=[0.3, 0.7]).tolist(),\n",
    "        np.random.choice([True, False], size=100, p=[0.4, 0.6]).tolist(),\n",
    "        np.random.choice([True, False], size=200, p=[0.2, 0.8]).tolist(),\n",
    "        \n",
    "        # Edge case: very long consecutive period\n",
    "        [False] * 50 + [True] * 150 + [False] * 50,\n",
    "        \n",
    "        # Multiple equal-length periods (test tie-breaking)\n",
    "        [True] * 10 + [False] * 5 + [True] * 10 + [False] * 5 + [True] * 10,\n",
    "        \n",
    "        # Single snow day in long period\n",
    "        [False] * 100 + [True] + [False] * 100,\n",
    "        \n",
    "        # Two equal periods at start and end\n",
    "        [True] * 15 + [False] * 20 + [True] * 15,\n",
    "    ]\n",
    "    \n",
    "    print(\"Comprehensive Function Test:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_passed = True\n",
    "    failed_cases = []\n",
    "    \n",
    "    for i, case in enumerate(test_cases):\n",
    "        arr = np.array(case, dtype=bool)\n",
    "        \n",
    "        orig = get_longest_consec_stretch_original(arr)\n",
    "        vect = get_longest_consec_stretch_vectorized(arr)\n",
    "        \n",
    "        match = orig == vect\n",
    "        status = \"✓ PASS\" if match else \"✗ FAIL\"\n",
    "        \n",
    "        print(f\"Test {i+1:2d}: {status} - Length: {len(case):3d}\")\n",
    "        \n",
    "        if len(case) <= 20:  # Show pattern for short cases\n",
    "            print(f\"  Pattern: {case}\")\n",
    "        else:  # Show summary for long cases\n",
    "            true_count = sum(case)\n",
    "            false_count = len(case) - true_count\n",
    "            print(f\"  Summary: {true_count} True, {false_count} False\")\n",
    "        \n",
    "        print(f\"  Original:   start={orig[0]:4d}, end={orig[1]:4d}, length={orig[2]:4d}\")\n",
    "        print(f\"  Vectorized: start={vect[0]:4d}, end={vect[1]:4d}, length={vect[2]:4d}\")\n",
    "        \n",
    "        if not match:\n",
    "            all_passed = False\n",
    "            failed_cases.append((i+1, case[:20] if len(case) > 20 else case, orig, vect))\n",
    "            print(f\"  *** MISMATCH! ***\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"SUMMARY:\")\n",
    "    print(f\"Total tests: {len(test_cases)}\")\n",
    "    print(f\"Passed: {len(test_cases) - len(failed_cases)}\")\n",
    "    print(f\"Failed: {len(failed_cases)}\")\n",
    "    \n",
    "    if all_passed:\n",
    "        print(\"\\n🎉 ALL TESTS PASSED! Functions are equivalent.\")\n",
    "        print(\"✅ SAFE TO USE the vectorized version.\")\n",
    "    else:\n",
    "        print(f\"\\n❌ {len(failed_cases)} TESTS FAILED! Functions are NOT equivalent.\")\n",
    "        print(\"❌ DO NOT USE the vectorized version yet.\")\n",
    "        print(\"\\nFailed test details:\")\n",
    "        for test_num, pattern, orig, vect in failed_cases:\n",
    "            print(f\"  Test {test_num}: Pattern={pattern} | Orig={orig} | Vect={vect}\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "# Performance benchmark with longer arrays\n",
    "def benchmark_functions():\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from numba import jit\n",
    "    \n",
    "    # Redefine functions for benchmarking\n",
    "    def get_longest_consec_stretch_original(arr):\n",
    "        max_len = 0\n",
    "        max_start = 0\n",
    "        max_end = 0\n",
    "        current_start = None\n",
    "        \n",
    "        for i, val in enumerate(arr):\n",
    "            if val:\n",
    "                if current_start is None:\n",
    "                    current_start = i\n",
    "            else:\n",
    "                if current_start is not None:\n",
    "                    length = i - current_start\n",
    "                    if length >= max_len:\n",
    "                        max_len = length\n",
    "                        max_start = current_start\n",
    "                        max_end = i\n",
    "                    current_start = None\n",
    "        \n",
    "        if current_start is not None:\n",
    "            length = len(arr) - current_start\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                max_start = current_start\n",
    "                max_end = len(arr)\n",
    "        \n",
    "        if max_len == 0:\n",
    "            return -32768, -32768, -32768\n",
    "        \n",
    "        return max_start, max_end, max_len\n",
    "    \n",
    "    @jit(nopython=True)\n",
    "    def get_longest_consec_stretch_vectorized(arr):\n",
    "        n = len(arr)\n",
    "        if n == 0:\n",
    "            return -32768, -32768, -32768\n",
    "        \n",
    "        max_len = 0\n",
    "        max_start = 0\n",
    "        max_end = 0\n",
    "        current_start = -1\n",
    "        \n",
    "        for i in range(n):\n",
    "            if arr[i]:\n",
    "                if current_start == -1:\n",
    "                    current_start = i\n",
    "            else:\n",
    "                if current_start != -1:\n",
    "                    length = i - current_start\n",
    "                    if length >= max_len:\n",
    "                        max_len = length\n",
    "                        max_start = current_start\n",
    "                        max_end = i\n",
    "                    current_start = -1\n",
    "        \n",
    "        if current_start != -1:\n",
    "            length = n - current_start\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                max_start = current_start\n",
    "                max_end = n\n",
    "        \n",
    "        if max_len == 0:\n",
    "            return -32768, -32768, -32768\n",
    "        \n",
    "        return max_start, max_end, max_len\n",
    "    \n",
    "    print(\"\\nPerformance Benchmark:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Test different array sizes\n",
    "    sizes = [46, 100, 500, 1000]  # Realistic MODIS sizes\n",
    "    iterations = [50000, 20000, 5000, 2000]  # Fewer iterations for larger arrays\n",
    "    \n",
    "    for size, iters in zip(sizes, iterations):\n",
    "        np.random.seed(42)  # Reproducible results\n",
    "        test_array = np.random.choice([True, False], size=size, p=[0.3, 0.7])\n",
    "        \n",
    "        # Warm up numba\n",
    "        _ = get_longest_consec_stretch_vectorized(test_array)\n",
    "        \n",
    "        # Benchmark original\n",
    "        start_time = time.time()\n",
    "        for _ in range(iters):\n",
    "            _ = get_longest_consec_stretch_original(test_array)\n",
    "        orig_time = time.time() - start_time\n",
    "        \n",
    "        # Benchmark vectorized\n",
    "        start_time = time.time()\n",
    "        for _ in range(iters):\n",
    "            _ = get_longest_consec_stretch_vectorized(test_array)\n",
    "        vect_time = time.time() - start_time\n",
    "        \n",
    "        speedup = orig_time / vect_time if vect_time > 0 else float('inf')\n",
    "        \n",
    "        print(f\"Array size {size:4d} ({iters:5d} iterations):\")\n",
    "        print(f\"  Original:   {orig_time:.4f}s ({orig_time/iters*1000:.3f}ms per call)\")\n",
    "        print(f\"  Vectorized: {vect_time:.4f}s ({vect_time/iters*1000:.3f}ms per call)\")\n",
    "        print(f\"  Speedup:    {speedup:.2f}x\")\n",
    "        print()\n",
    "\n",
    "# Run the comprehensive test\n",
    "print(\"Running comprehensive test...\")\n",
    "test_passed = comprehensive_test_vectorized_function()\n",
    "\n",
    "if test_passed:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    benchmark_functions()\n",
    "else:\n",
    "    print(\"\\nSkipping benchmark due to failed tests.\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL RESULT: {'✅ SAFE TO USE' if test_passed else '❌ NEEDS FIXING'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individal tile investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to figure out why doesnt SAD start early in antartica? is filling happening correctly? \n",
    "# # and then what is going on at the north pole ish during start of WY as well?\n",
    "# v=16\n",
    "# h=16\n",
    "# #v=0\n",
    "# WY_start = 2014\n",
    "# WY_end = 2024\n",
    "\n",
    "# hemisphere = \"northern\" if v < 9 else \"southern\"\n",
    "\n",
    "\n",
    "# if hemisphere == \"northern\":\n",
    "#     modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "#         vertical_tile=v,\n",
    "#         horizontal_tile=h,\n",
    "#         start_date=f\"{WY_start-2}-10-01\",\n",
    "#         end_date=f\"{WY_end}-09-30\",\n",
    "#         #chunks={},\n",
    "#         #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "#         chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "#     ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "# else:\n",
    "#     modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "#         vertical_tile=v,\n",
    "#         horizontal_tile=h,\n",
    "#         start_date=f\"{WY_start-1}-04-01\",\n",
    "#         end_date=f\"{WY_end+1}-03-31\",\n",
    "#         #chunks={},\n",
    "#         #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "#         chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "#     ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "# modis_snow_da.coords[\"water_year\"] = (\n",
    "#     \"time\",\n",
    "#     pd.to_datetime(modis_snow_da.time).map(\n",
    "#         lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere)\n",
    "#     ),\n",
    "# )\n",
    "# modis_snow_da.coords[\"DOWY\"] = (\n",
    "#     \"time\",\n",
    "#     pd.to_datetime(modis_snow_da.time).map(\n",
    "#         lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "# modis_snow_da\n",
    "\n",
    "# modis_snow_da.sel(time='2020-03-29').plot.imshow()\n",
    "\n",
    "# no_decision_and_night_counts = modis_snow_da.where(lambda x: (x == 1) | (x == 11)).count(dim=[\"x\",\"y\"]).compute()\n",
    "\n",
    "# scenes_with_polar_night = no_decision_and_night_counts > 50\n",
    "# # then for every true value, change the value that comes directly before and after it to 1 as well\n",
    "# scenes_with_polar_night_buffered = (scenes_with_polar_night.shift(time=-1).fillna(0) | scenes_with_polar_night | scenes_with_polar_night.shift(time=1).fillna(0)).astype(int)\n",
    "# backward_check = scenes_with_polar_night_buffered.rolling(time=4, center=False).sum() >= 4  # forward-looking\n",
    "# forward_check = scenes_with_polar_night_buffered[::-1].rolling(time=4, center=False).sum()[::-1] >= 4  # backward-looking\n",
    "# center_check = scenes_with_polar_night_buffered.rolling(time=4, center=True).sum() >= 4  # center-looking\n",
    "\n",
    "# # A position should be kept if it's part of a sequence of 4+ when looking in any direction\n",
    "# scenes_with_polar_night_buffered_filtered = scenes_with_polar_night_buffered.where(\n",
    "#     backward_check | forward_check | center_check,\n",
    "#     other=0\n",
    "# ).astype(bool)\n",
    "# scenes_with_polar_night_buffered_filtered\n",
    "# # count the occurance of 1s and 11s in modis_snow_da which is dask array\n",
    "\n",
    "\n",
    "# # f,ax=plt.subplots(figsize=(20,7))\n",
    "# # no_decision_and_night_counts.plot()\n",
    "# # ax.axvline(x=pd.to_datetime('2020-09-05'), color='red', linestyle='--')\n",
    "# # binary_ax = ax.twinx()\n",
    "# # scenes_with_polar_night_buffered.plot(ax=binary_ax, color='orange')\n",
    "# # scenes_with_polar_night_buffered_filtered.plot(ax=binary_ax, color='green')\n",
    "# # (scenes_with_polar_night_buffered_filtered-scenes_with_polar_night_buffered).plot(ax=binary_ax, color='purple', linestyle='--')\n",
    "\n",
    "# scenes_with_polar_night_buffered_filtered.sel(time=slice('2020-03-13', '2020-09-30'))\n",
    "\n",
    "\n",
    "# modis_snow_da # not working as intended? CHECK LOGIC\n",
    "\n",
    "# values,counts=np.unique(modis_snow_da.sel(time='2020-03-29'), return_counts=True)\n",
    "# for v,c in zip(values,counts):\n",
    "#     print(f\"{v}: {c}\")\n",
    "\n",
    "# # now for time steps where no_decision_and_night_counts is greater than 10, change the value of modis_snow_da from 25 to 255\n",
    "# # modis_snow_da = modis_snow_da.where(\n",
    "# #     ~((modis_snow_da == 25) & (no_decision_and_night_counts > 10)), other=255\n",
    "# # )\n",
    "# # modis_snow_da\n",
    "\n",
    "# modis_snow_da.sel(time='2020-03-29').plot.imshow()\n",
    "\n",
    "# values,counts=np.unique(modis_snow_da.sel(time='2020-03-29'), return_counts=True)\n",
    "# for v,c in zip(values,counts):\n",
    "#     print(f\"{v}: {c}\")\n",
    "\n",
    "\n",
    "# effective_snow_da = modis_masking.binarize_with_cloud_filling(modis_snow_da)\n",
    "# effective_snow_da\n",
    "\n",
    "# effective_snow_da.sel(time=slice('2020-03-29', '2020-09-30')).plot.imshow(col='time',col_wrap=5,figsize=(10, 10))\n",
    "\n",
    "# effective_snow_complete_wys_da = modis_masking.align_wy_start(effective_snow_da, hemisphere=hemisphere)\n",
    "# effective_snow_complete_wys_da\n",
    "\n",
    "\n",
    "# seasonal_snow_cover_ds = effective_snow_complete_wys_da.groupby('water_year').apply(modis_masking.get_max_consec_snow_days_SAD_SDD_one_WY).compute()\n",
    "# seasonal_snow_cover_ds\n",
    "\n",
    "# seasonal_snow_cover_ds['max_consec_snow_days'].sel(water_year=2020).plot.imshow(vmin=0, vmax=365, cmap='viridis')\n",
    "# seasonal_snow_cover_ds['SAD_DOWY'].sel(water_year=2020).plot.imshow(vmin=0, vmax=365, cmap='viridis')\n",
    "\n",
    "# tile_processing_list = [\n",
    "#     \"h15_v15\",\n",
    "#     \"h15_v16\",\n",
    "#     \"h15_v17\",\n",
    "#     \"h16_v16\",\n",
    "#     \"h16_v17\",\n",
    "#     \"h17_v16\",\n",
    "#     \"h17_v17\",\n",
    "#     \"h16_v0\",\n",
    "#     \"h16_v1\",\n",
    "#     \"h16_v2\",\n",
    "#     \"h17_v0\",\n",
    "#     \"h17_v1\",\n",
    "#     \"h17_v2\",\n",
    "#     \"h17_v3\",\n",
    "#     \"h14_v1\",\n",
    "#     \"h14_v2\",\n",
    "#     \"h14_v3\",\n",
    "#     \"h14_v4\",\n",
    "#     \"h15_v1\",\n",
    "#     \"h15_v2\",]\n",
    "# tile_processing_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other approaches (code graveyard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed_tiles = []\n",
    "# results = []\n",
    "\n",
    "# #processed_tiles_list_initial = zarr.open(store).attrs['processed_tiles']\n",
    "# for tile in tqdm.tqdm(tile_processing_list[0:20]):\n",
    "#     result = client.submit(process_tile, tile, store)\n",
    "#     results.append(result)\n",
    "\n",
    "# results[0].result()\n",
    "# tile_processing_list\n",
    "\n",
    "# result = client.submit(process_tile, tile_processing_list[0], store)\n",
    "# result\n",
    "\n",
    "# failed_tiles = []\n",
    "# futures = {}\n",
    "\n",
    "# # Get initially processed tiles\n",
    "# processed_tiles_list_initial = zarr.open(store).attrs['processed_tiles']\n",
    "\n",
    "# # Submit all unprocessed tiles to the cluster\n",
    "# for tile in tqdm.tqdm(tile_processing_list, desc=\"Submitting tasks\"):\n",
    "#     if tile in processed_tiles_list_initial:\n",
    "#         print(f\"Tile {tile} already processed, skipping\")\n",
    "#         continue\n",
    "    \n",
    "#     future = client.submit(process_tile, tile, store)\n",
    "#     futures[future] = tile\n",
    "\n",
    "# print(f\"Submitted {len(futures)} tiles for processing\")\n",
    "\n",
    "# # Process completed futures as they finish\n",
    "# from dask.distributed import as_completed\n",
    "\n",
    "# for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=\"Processing tiles\"):\n",
    "#     tile = futures[future]\n",
    "    \n",
    "#     try:\n",
    "#         result = future.result()\n",
    "        \n",
    "#         if result == True:\n",
    "#             # Update processed tiles list in zarr store\n",
    "#             with zarr.open(store) as zarr_store:\n",
    "#                 processed_tile_list = zarr_store.attrs['processed_tiles']\n",
    "#                 processed_tile_list.append(tile)\n",
    "#                 zarr_store.attrs['processed_tiles'] = processed_tile_list\n",
    "            \n",
    "#             print(f\"Tile {tile} SUCCESS, added to processed_list attribute\")\n",
    "#         else:\n",
    "#             print(f\"Tile {tile} FAIL, adding to failed list\")\n",
    "#             failed_tiles.append(tile)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Tile {tile} FAILED with exception: {str(e)}\")\n",
    "#         failed_tiles.append(tile)\n",
    "\n",
    "# # Restart cluster after all processing is complete\n",
    "# if len(futures) > 0:\n",
    "#     client.restart(wait_for_workers=True)\n",
    "\n",
    "# # Final status report\n",
    "# if failed_tiles:\n",
    "#     print(\"Run this cell again. The following tiles could not be processed:\")\n",
    "#     for tile in failed_tiles:\n",
    "#         print(tile)\n",
    "# else:\n",
    "#     print(\"Now consolidating metadata...\")\n",
    "#     zarr.consolidate_metadata(store)\n",
    "#     print(\"All tiles processed successfully!!!\")\n",
    "\n",
    "\n",
    "# failed_tiles = []\n",
    "\n",
    "# processed_tiles_list_initial = zarr.open(store).attrs['processed_tiles']\n",
    "# for tile in tqdm.tqdm(tile_processing_list):\n",
    "    \n",
    "#     if tile in processed_tiles_list_initial:\n",
    "#         print(f\"Tile {tile} already processed, skipping\")\n",
    "#         continue\n",
    "        \n",
    "#     result = process_tile(tile, store)\n",
    "\n",
    "#     if result == True:\n",
    "\n",
    "#         with zarr.open(store) as zarr_store:\n",
    "#             processed_tile_list = zarr_store.attrs['processed_tiles']\n",
    "#             processed_tile_list.append(tile)\n",
    "#             zarr_store.attrs['processed_tiles'] = processed_tile_list\n",
    "\n",
    "#         print(f\"Tile {tile} SUCCESS, added to processed_list attribute\")\n",
    "\n",
    "#         client.restart(wait_for_workers=True)\n",
    "\n",
    "#     else:\n",
    "#         print(f\"Tile {tile} FAIL, adding to failed list\")\n",
    "#         failed_tiles.append(tile)\n",
    "\n",
    "\n",
    "# if failed_tiles:\n",
    "#     print(\"Run this cell again. The following tiles could not be processed:\")\n",
    "#     for tile in failed_tiles:\n",
    "#         print(tile)\n",
    "# else:\n",
    "#     print(\"Now consolidating metadata...\")\n",
    "#     zarr.consolidate_metadata(store)\n",
    "#     print(\"All tiles processed successfully!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tile(tile,bad_pixel_thresh=500):\n",
    "#     h, v = (int(part[1:]) for part in tile.split('_'))\n",
    "\n",
    "#     hemisphere = \"northern\" if v < 9 else \"southern\"\n",
    "\n",
    "#     # logger.info(f\"Fetching MODIS data for tile {tile}\")\n",
    "#     if hemisphere == \"northern\":\n",
    "#         modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "#             vertical_tile=v,\n",
    "#             horizontal_tile=h,\n",
    "#             start_date=f\"{WY_start-2}-10-01\", # normally should be WY_start-1, but we want to get data from WY before, as October 1st acquistion in NH already has fill values\n",
    "#             end_date=f\"{WY_end}-09-30\",\n",
    "#             #chunks={},\n",
    "#             #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "#             chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "#         ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "#     else:\n",
    "#         modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "#             vertical_tile=v,\n",
    "#             horizontal_tile=h,\n",
    "#             start_date=f\"{WY_start-1}-04-01\", # normally should be WY_start, but we want to get data from WY before, as October 1st acquistion in NH already has fill values\n",
    "#             end_date=f\"{WY_end+1}-03-31\",\n",
    "#             #chunks={},\n",
    "#             #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "#             chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "#         ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "#     # logger.info(f\"Processing MODIS data for tile {tile}\")\n",
    "#     modis_snow_da.coords[\"water_year\"] = (\n",
    "#         \"time\",\n",
    "#         pd.to_datetime(modis_snow_da.time).map(\n",
    "#             lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere)\n",
    "#         ),\n",
    "#     )\n",
    "#     modis_snow_da.coords[\"DOWY\"] = (\n",
    "#         \"time\",\n",
    "#         pd.to_datetime(modis_snow_da.time).map(\n",
    "#             lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     value1_da = modis_snow_da.where(lambda x: x == 1).count(dim=[\"x\",\"y\"])\n",
    "#     value11_da = modis_snow_da.where(lambda x: x == 11).count(dim=[\"x\",\"y\"])\n",
    "#     value25_da = modis_snow_da.where(lambda x: x == 25).count(dim=[\"x\",\"y\"])\n",
    "#     value200_da = modis_snow_da.where(lambda x: x == 200).count(dim=[\"x\",\"y\"])\n",
    "#     no_decision_and_night_counts = modis_snow_da.where(lambda x: (x == 1) | (x == 11)).count(dim=[\"x\",\"y\"])\n",
    "\n",
    "#     land_area_da = value200_da + value25_da\n",
    "#     max_land_pixels = land_area_da.max(dim='time')\n",
    "#     bad_pixel_thresh = int(0.05*int(max_land_pixels))\n",
    "\n",
    "#     scenes_with_polar_night = no_decision_and_night_counts > bad_pixel_thresh\n",
    "#     scenes_with_polar_night_buffered = (scenes_with_polar_night.shift(time=-1).fillna(0) | scenes_with_polar_night | scenes_with_polar_night.shift(time=1).fillna(0)).astype(int)\n",
    "#     backward_check = scenes_with_polar_night_buffered.rolling(time=4, center=False).sum() >= 4  # forward-looking\n",
    "#     forward_check = scenes_with_polar_night_buffered[::-1].rolling(time=4, center=False).sum()[::-1] >= 4 # backward-looking\n",
    "#     center_check = scenes_with_polar_night_buffered.rolling(time=4, center=True).sum() >= 4  # center-looking\n",
    "\n",
    "#     # A position should be kept if it's part of a sequence of 4+ when looking in any direction\n",
    "#     scenes_with_polar_night_buffered_filtered = scenes_with_polar_night_buffered.where(\n",
    "#         backward_check | forward_check | center_check,\n",
    "#         other=0\n",
    "#     ).astype(bool).chunk(dict(time=-1))\n",
    "\n",
    "#     scenes_with_polar_night_buffered_filtered_complete = (\n",
    "#         scenes_with_polar_night_buffered_filtered.where(lambda x: x == 1)\n",
    "#         .interpolate_na(dim=\"time\", method=\"nearest\", max_gap=pd.Timedelta(days=80))\n",
    "#         .where(lambda x: x == 1, other=0)\n",
    "#         .astype(bool)\n",
    "#     )\n",
    "\n",
    "#     all_variables = xr.Dataset({\n",
    "#         \"value1_da\": value1_da,\n",
    "#         \"value11_da\": value11_da,\n",
    "#         \"value25_da\": value25_da,\n",
    "#         \"no_decision_and_night_counts\": no_decision_and_night_counts,\n",
    "#         \"scenes_with_polar_night\": scenes_with_polar_night,\n",
    "#         \"scenes_with_polar_night_buffered\": scenes_with_polar_night_buffered,\n",
    "#         \"scenes_with_polar_night_buffered_filtered\": scenes_with_polar_night_buffered_filtered,\n",
    "#         \"scenes_with_polar_night_buffered_filtered_complete\": scenes_with_polar_night_buffered_filtered_complete\n",
    "#     })\n",
    "\n",
    "#     all_variables.coords['tile'] = tile\n",
    "#     all_variables.coords['bad_pixel_thresh'] = bad_pixel_thresh\n",
    "\n",
    "#     return all_variables\n",
    "\n",
    "# tile_processing_list_filtered[0]\n",
    "# test_ds = test_tile(tile_processing_list_filtered[0], bad_pixel_thresh=500).compute()\n",
    "# test_ds\n",
    "\n",
    "\n",
    "\n",
    "# f,ax= plt.subplots(figsize=(12, 7))\n",
    "# test_ds['value1_da'].plot(ax=ax,label='Value 1 (no decision) count', color='blue')\n",
    "# test_ds['value11_da'].plot(ax=ax,label='Value 11 (night) count', color='orange')\n",
    "# test_ds['value25_da'].plot(ax=ax,label='Value 25 (no snow) count', color='green')\n",
    "# test_ds['no_decision_and_night_counts'].plot(ax=ax,label='No decision and night counts', color='purple')\n",
    "# binary_ax = ax.twinx()\n",
    "# test_ds['scenes_with_polar_night_buffered_filtered'].plot(ax=binary_ax, label='Scenes with polar night',color='black')\n",
    "# ax.legend()\n",
    "# f.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# def create_test_dataset(tile_processing_list_filtered):\n",
    "#     futures = [client.submit(test_tile, tile) for tile in tile_processing_list_filtered]\n",
    "#     results = []\n",
    "#     for future in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             results.append(result)\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error processing tile {future.key}: {str(e)}\")\n",
    "#             logging.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "    \n",
    "#     return xr.concat(results, dim=\"tile\")\n",
    "\n",
    "\n",
    "# full_test_ds = create_test_dataset(tile_processing_list_filtered).compute()\n",
    "# full_test_ds\n",
    "\n",
    "\n",
    "# # sort full_test_ds by tile, but specifically by increasing v value. remember you CANNOT use .map with a dataarray bc of error: AttributeError: 'DataArray' object has no attribute 'map'\n",
    "# v_values = []\n",
    "# for tile in full_test_ds.tile.values:\n",
    "#     # Split the tile string and extract v value\n",
    "#     parts = tile.split('_')\n",
    "#     v_part = parts[1]  # e.g., 'v17'\n",
    "#     v_value = int(v_part[1:])  # Remove 'v' and convert to int\n",
    "#     v_values.append(v_value)\n",
    "\n",
    "# # Convert to numpy array for sorting\n",
    "# v_values = np.array(v_values)\n",
    "\n",
    "# full_test_sorted_ds = full_test_ds.isel(tile=np.argsort(v_values)[::-1])\n",
    "# full_test_sorted_ds\n",
    "\n",
    "\n",
    "# # investigate h9_v2 -> no polar night ever\n",
    "# # # \n",
    "# # f,ax=plt.subplots(figsize=(12,7))\n",
    "# # for tile in full_test_ds.tile.values[0:3]:\n",
    "# #     print(f\"Tile: {tile}\")\n",
    "# #     full_test_ds.sel(tile=tile)['scenes_with_polar_night_buffered_filtered'].plot(ax=ax, label=tile)\n",
    "\n",
    "# # f,ax= plt.subplots(figsize=(12, 7))\n",
    "# # test_ds['value1_da'].plot(ax=ax,label='Value 1 (no decision) count', color='blue')\n",
    "# # test_ds['value11_da'].plot(ax=ax,label='Value 11 (night) count', color='orange')\n",
    "# # test_ds['value25_da'].plot(ax=ax,label='Value 25 (no snow) count', color='green')\n",
    "# # test_ds['no_decision_and_night_counts'].plot(ax=ax,label='No decision and night counts', color='purple')\n",
    "# # binary_ax = ax.twinx()\n",
    "# # test_ds['scenes_with_polar_night_buffered_filtered'].plot(ax=binary_ax, label='Scenes with polar night',color='black')\n",
    "# # ax.legend()\n",
    "# # f.tight_layout()\n",
    "\n",
    "# # h=11\n",
    "# # v=2\n",
    "\n",
    "# # hemisphere = \"northern\" if v < 9 else \"southern\"\n",
    "\n",
    "# # # logger.info(f\"Fetching MODIS data for tile {tile}\")\n",
    "# # if hemisphere == \"northern\":\n",
    "# #     modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "# #         vertical_tile=v,\n",
    "# #         horizontal_tile=h,\n",
    "# #         start_date=f\"{WY_start-2}-10-01\", # normally should be WY_start-1, but we want to get data from WY before, as October 1st acquistion in NH already has fill values\n",
    "# #         end_date=f\"{WY_end}-09-30\",\n",
    "# #         #chunks={},\n",
    "# #         #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "# #         chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "# #     ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "# # else:\n",
    "# #     modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "# #         vertical_tile=v,\n",
    "# #         horizontal_tile=h,\n",
    "# #         start_date=f\"{WY_start-1}-04-01\", # normally should be WY_start, but we want to get data from WY before, as October 1st acquistion in NH already has fill values\n",
    "# #         end_date=f\"{WY_end+1}-03-31\",\n",
    "# #         #chunks={},\n",
    "# #         #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "# #         chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "# #     ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "# # # logger.info(f\"Processing MODIS data for tile {tile}\")\n",
    "# # modis_snow_da.coords[\"water_year\"] = (\n",
    "# #     \"time\",\n",
    "# #     pd.to_datetime(modis_snow_da.time).map(\n",
    "# #         lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere)\n",
    "# #     ),\n",
    "# # )\n",
    "# # modis_snow_da.coords[\"DOWY\"] = (\n",
    "# #     \"time\",\n",
    "# #     pd.to_datetime(modis_snow_da.time).map(\n",
    "# #         lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)\n",
    "# #     ),\n",
    "# # )\n",
    "\n",
    "# # modis_snow_da\n",
    "\n",
    "# # value1_da = modis_snow_da.where(lambda x: x == 1).count(dim=[\"x\",\"y\"]).compute()\n",
    "# # value1_da\n",
    "\n",
    "# # value11_da = modis_snow_da.where(lambda x: x == 11).count(dim=[\"x\",\"y\"]).compute()\n",
    "# # value11_da\n",
    "\n",
    "# # no_decision_and_night_counts = modis_snow_da.where(lambda x: (x == 1) | (x == 11)).count(dim=[\"x\",\"y\"]).compute()\n",
    "# # no_decision_and_night_counts\n",
    "\n",
    "\n",
    "# # f,ax=plt.subplots(figsize=(20,7))\n",
    "# # no_decision_and_night_counts.plot(ax=ax, color='orange', label='No Decision and Night Count')\n",
    "# # #ax.axvline(x=pd.to_datetime('2020-09-05'), color='red', linestyle='--')\n",
    "# # small_ax = ax.twinx()\n",
    "# # value1_da.plot(ax=ax, color='green', label='Value 1 Count')\n",
    "# # value11_da.plot(ax=ax, color='blue', label='Value 11 Count')\n",
    "\n",
    "\n",
    "# # scenes_with_polar_night = no_decision_and_night_counts > 500\n",
    "\n",
    "# # # then for every true value, change the value that comes directly before and after it to 1 as well\n",
    "# # scenes_with_polar_night_buffered = (scenes_with_polar_night.shift(time=-1).fillna(0) | scenes_with_polar_night | scenes_with_polar_night.shift(time=1).fillna(0)).astype(int)\n",
    "# # backward_check = scenes_with_polar_night_buffered.rolling(time=4, center=False).sum() >= 4  # forward-looking\n",
    "# # forward_check = scenes_with_polar_night_buffered[::-1].rolling(time=4, center=False).sum()[::-1] >= 4 # backward-looking\n",
    "# # center_check = scenes_with_polar_night_buffered.rolling(time=4, center=True).sum() >= 4  # center-looking\n",
    "\n",
    "# # # A position should be kept if it's part of a sequence of 4+ when looking in any direction\n",
    "# # scenes_with_polar_night_buffered_filtered = scenes_with_polar_night_buffered.where(\n",
    "# #     backward_check | forward_check | center_check,\n",
    "# #     other=0\n",
    "# # ).astype(bool)\n",
    "\n",
    "# # f,ax=plt.subplots(figsize=(20,7))\n",
    "# # no_decision_and_night_counts.plot()\n",
    "# # #ax.axvline(x=pd.to_datetime('2020-09-05'), color='red', linestyle='--')\n",
    "# # binary_ax = ax.twinx()\n",
    "# # scenes_with_polar_night_buffered.plot(ax=binary_ax, color='orange')\n",
    "# # scenes_with_polar_night_buffered_filtered.plot(ax=binary_ax, color='green')\n",
    "# # (scenes_with_polar_night_buffered_filtered-scenes_with_polar_night_buffered).plot(ax=binary_ax, color='purple', linestyle='--')\n",
    "\n",
    "\n",
    "# # if (v>=15) | (v<=2):# if north or south pole, remove strange no snow artifacts around time of darkness (check to see if darkness in the scene?)\n",
    "# #         # if a scene contains significant no decision (1) / night (11) values, change no snow (25) to fill (255)?\n",
    "            \n",
    "# #             no_decision_and_night_counts = modis_snow_da.where(lambda x: (x == 1) | (x == 11)).count(dim=[\"x\",\"y\"]).compute()\n",
    "\n",
    "# #             scenes_with_polar_night = no_decision_and_night_counts > 50\n",
    "# #             # then for every true value, change the value that comes directly before and after it to 1 as well\n",
    "# #             scenes_with_polar_night_buffered = (scenes_with_polar_night.shift(time=-1).fillna(0) | scenes_with_polar_night | scenes_with_polar_night.shift(time=1).fillna(0)).astype(int)\n",
    "# #             backward_check = scenes_with_polar_night_buffered.rolling(time=4, center=False).sum() >= 4  # forward-looking\n",
    "# #             forward_check = scenes_with_polar_night_buffered[::-1].rolling(time=4, center=False).sum()[::-1] >= 4  # backward-looking\n",
    "# #             center_check = scenes_with_polar_night_buffered.rolling(time=4, center=True).sum() >= 4  # center-looking\n",
    "\n",
    "# #             # A position should be kept if it's part of a sequence of 4+ when looking in any direction\n",
    "# #             scenes_with_polar_night_buffered_filtered = scenes_with_polar_night_buffered.where(\n",
    "# #                 backward_check | forward_check | center_check,\n",
    "# #                 other=0\n",
    "# #             ).astype(bool)\n",
    "\n",
    "# #             modis_snow_da = modis_snow_da.where(\n",
    "# #                 ~((modis_snow_da == 25) & (scenes_with_polar_night_buffered_filtered)), other=255\n",
    "# #             )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serverless approach (this got close, couldn't push it across finish line though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.coiled.io/user_guide/functions.html\n",
    "# inspired by: https://github.com/earth-mover/serverless-datacube-demo/blob/main/src/lib.py\n",
    "\n",
    "# maybe another option: https://xarray.dev/blog/cubed-xarray\n",
    "# @coiled.function(\n",
    "#     n_workers=50,\n",
    "#     cpu=4,\n",
    "#     #threads_per_worker=8,\n",
    "#     memory=\"16GiB\",\n",
    "#     spot_policy=\"spot\",\n",
    "#     region=\"westeurope\",\n",
    "#     environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "#     keepalive=\"5m\",\n",
    "#     workspace=\"azure\"\n",
    "# )\n",
    "# # def process_chunks(tile_list, store):\n",
    "# #     odc.stac.configure_rio(cloud_defaults=True)\n",
    "# #     results = []\n",
    "# #     for _, tile in tile_list:\n",
    "# #         h = tile['h']\n",
    "# #         v = tile['v']\n",
    "# #         result = process_and_write_tile(h, v, store, serverless=False)\n",
    "# #         results.append(result)\n",
    "# #     return results\n",
    "# def process_chunk(tile, store):\n",
    "#     odc.stac.configure_rio(cloud_defaults=True)\n",
    "#     #with dask.config.set(pool=concurrent.futures.ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#     process = process_and_write_tile(tile, store, serverless=False)\n",
    "#     return process\n",
    "\n",
    "\n",
    "# def spawn_coiled_jobs(\n",
    "#     modis_grid_land_list, store):\n",
    "#     h_list = [tile['h'] for _, tile in modis_grid_land_list]\n",
    "#     v_list = [tile['v'] for _, tile in modis_grid_land_list]\n",
    "#     results = list(\n",
    "#         tqdm.tqdm(\n",
    "#             process_chunk.map(\n",
    "#                 h_list, \n",
    "#                 v_list,\n",
    "#                 store=store,\n",
    "#                 retries=5\n",
    "#             ),\n",
    "#             total=len(h_list),\n",
    "#             desc=\"Jobs Completed\",\n",
    "#         )\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "# # def spawn_coiled_jobs(modis_grid_land_list, store, batch_size=10):\n",
    "# #     batches = [modis_grid_land_list[i:i+batch_size] for i in range(0, len(modis_grid_land_list), batch_size)]\n",
    "# #     results = list(\n",
    "# #         tqdm.tqdm(\n",
    "# #             process_chunks.map(\n",
    "# #                 batches,\n",
    "# #                 store=store,\n",
    "# #                 retries=5\n",
    "# #             ),\n",
    "# #             total=len(batches),\n",
    "# #             desc=\"Batch completed\",\n",
    "# #         )\n",
    "# #     )\n",
    "# #     return [item for sublist in results for item in sublist]\n",
    "\n",
    "# #results = spawn_coiled_jobs(modis_grid_land_list, store)\n",
    "# #results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#futures = []\n",
    "# # for _, tile in tqdm.tqdm(modis_grid_land_list):\n",
    "# #     h = tile['h']\n",
    "# #     v = tile['v']\n",
    "# #     try:\n",
    "# #         process_and_write_tile(h, v, store)\n",
    "# #         print(f\"Tile h{h}_v{v} processed and written\")\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Error processing tile h{h}_v{v}: {str(e)}\")\n",
    "# #         print(f\"Traceback: {traceback.format_exc()}\")\n",
    "# #         # maybe append to a list of all tiles that need to be rerun\n",
    "# #     #future = client.submit(process_and_write_tile, h, v, store)\n",
    "# #     #futures.append(future)\n",
    "# # #results = wait(futures)\n",
    "# # \n",
    "# # for future in futures:\n",
    "#     try:\n",
    "#         result = future.result()\n",
    "#         print(result)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Task failed: {str(e)}\")\n",
    "#         print(f\"Traceback: {future.traceback()}\")\n",
    "# \n",
    "# \n",
    "# # client.close()\n",
    "# cluster.close()\n",
    "# \n",
    "#     #seasonal_snow_presence.drop_vars('spatial_ref').chunk({'water_year':1,'y':2400,'x':2400}).to_zarr(store, region={'water_year':water_year_slice,'y':y_slice,'x':x_slice}, mode=\"r+\")\n",
    "\n",
    "# if serverless:\n",
    "#     print(f'running serverless mode, using threadpoolexecutor...')\n",
    "#     with dask.config.set(pool=concurrent.futures.ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#         for var in ['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']:\n",
    "#             data = seasonal_snow_presence[var].values\n",
    "#             root[var][:,y_start:y_end,x_start:x_end] = data\n",
    "# else:\n",
    "#     for var in ['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']:\n",
    "#         data = seasonal_snow_presence[var].values\n",
    "#         root[var][:,y_start:y_end,x_start:x_end] = data\n",
    "\n",
    "# root[:, time_slice, y_slice, x_slice] = data\n",
    "\n",
    "\n",
    "    #root[var][time_slice, y_slice, x_slice] = data\n",
    "\n",
    "# if data.shape[0] == 9 and data.shape[1] == 2400 and data.shape[2] == 2400:\n",
    "#    print(f'transpose necessary h{h}_v{v}')\n",
    "#    data = np.transpose(data, (1, 2, 0))\n",
    "\n",
    "\n",
    "# store.flush()\n",
    "\n",
    "\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:  # Adjust number as needed\n",
    "#     futures = [executor.submit(process_and_write_tile, h, v, azure_zarr_path) \n",
    "#                for h, v in modis_grid_land_list]\n",
    "\n",
    "# def process_batch(batch):\n",
    "#     results = []\n",
    "#     for h, v in batch:\n",
    "#         results.append(process_and_write_tile(h, v, azure_zarr_path))\n",
    "#     return results\n",
    "\n",
    "# batch_size = 10  # Adjust based on your workload\n",
    "# batches = [modis_grid_land_list[i:i+batch_size] for i in range(0, len(modis_grid_land_list), batch_size)]\n",
    "# futures = client.map(process_batch, batches)\n",
    "\n",
    "\n",
    "\n",
    "# def create_azure_zarr_store(connection_string, container_name, zarr_store_path):\n",
    "#     blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "#     container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "#     class AzureBlobStore(zarr.ABSStore):\n",
    "#         def __init__(self, container_client, prefix):\n",
    "#             self.container_client = container_client\n",
    "#             self.prefix = prefix\n",
    "#             self.client = container_client  # Add this line\n",
    "\n",
    "#         def __getitem__(self, key):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             return blob_client.download_blob().readall()\n",
    "\n",
    "#         def __setitem__(self, key, value):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             blob_client.upload_blob(value, overwrite=True)\n",
    "\n",
    "#         def __contains__(self, key):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             return blob_client.exists()\n",
    "\n",
    "#         def __delitem__(self, key):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             blob_client.delete_blob()\n",
    "\n",
    "#         def rmdir(self, path):\n",
    "#             dir_path = self.prefix\n",
    "#             if path:\n",
    "#                 dir_path += \"/\" + path\n",
    "#             dir_path += \"/\"\n",
    "#             blobs_to_delete = self.container_client.list_blobs(\n",
    "#                 name_starts_with=dir_path\n",
    "#             )\n",
    "#             for blob in blobs_to_delete:\n",
    "#                 self.container_client.delete_blob(blob)\n",
    "\n",
    "#     store = AzureBlobStore(container_client, zarr_store_path)\n",
    "\n",
    "#     root = zarr.open(store, mode=\"w\")\n",
    "#     # root.create_dataset('SAD_DOWY', shape=(36 * 2400, 18 * 2400, len(range(WY_start, WY_end + 1))), chunks=(2400, 2400, 1), dtype='i2')\n",
    "#     # root.create_dataset('SDD_DOWY', shape=(36 * 2400, 18 * 2400, len(range(WY_start, WY_end + 1))), chunks=(2400, 2400, 1), dtype='i2')\n",
    "#     # root.create_dataset('max_consec_snow_days', shape=(36 * 2400, 18 * 2400, len(range(WY_start, WY_end + 1))), chunks=(2400, 2400, 1), dtype='i2')\n",
    "#     water_years = list(range(WY_start, WY_end + 1))\n",
    "\n",
    "#     num_years = len(water_years)\n",
    "\n",
    "#     compressor = numcodecs.Blosc(\n",
    "#         cname=\"zstd\", clevel=3, shuffle=numcodecs.Blosc.SHUFFLE\n",
    "#     )\n",
    "\n",
    "#     # Create datasets\n",
    "#     for var in ['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']:\n",
    "#         dataset = root.create_dataset(\n",
    "#             var,\n",
    "#             shape=(num_years, 18 * 2400, 36 * 2400),\n",
    "#             chunks=(1, 2400, 2400),\n",
    "#             dtype=\"i2\",\n",
    "#             compressor=compressor,\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "#         # Add dimension names as attributes\n",
    "\n",
    "#     #root.create_dataset(\"water_year\", data=water_years, shape=(num_years,), dtype=\"i2\")\n",
    "#     # root[\"time\"].attrs[\n",
    "#     #     \"description\"\n",
    "#     # ] = \"Water year. In northern hemisphere, water year starts on October 1st and ends on September 30th. For the southern hemisphere, water year starts on April 1st and ends on March 31st. For example, in the northern hemisphere water year 2015 starts on October 1st, 2014 and ends on September 30th, 2015, and in the southern hemisphere water year 2015 starts on April 1st, 2015 and ends on March 31st, 2016.\"\n",
    "\n",
    "\n",
    "#     return f\"azure://{container_name}/{zarr_store_path}\"\n",
    "\n",
    "# from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# class AzureBlobStore(zarr.ABSStore):\n",
    "#     def __init__(self, container_client, prefix):\n",
    "#         self.container_client = container_client\n",
    "#         self.prefix = prefix\n",
    "#         self.client = container_client  # Add this line\n",
    "\n",
    "#     def __getitem__(self, key):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         return blob_client.download_blob().readall()\n",
    "\n",
    "#     def __setitem__(self, key, value):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         blob_client.upload_blob(value, overwrite=True)\n",
    "\n",
    "#     def __contains__(self, key):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         return blob_client.exists()\n",
    "\n",
    "#     def __delitem__(self, key):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         blob_client.delete_blob()\n",
    "\n",
    "#     def rmdir(self, path):\n",
    "#         dir_path = self.prefix\n",
    "#         if path:\n",
    "#             dir_path += \"/\" + path\n",
    "#         dir_path += \"/\"\n",
    "#         blobs_to_delete = self.container_client.list_blobs(\n",
    "#             name_starts_with=dir_path\n",
    "#         )\n",
    "#         for blob in blobs_to_delete:\n",
    "#             self.container_client.delete_blob(blob)\n",
    "\n",
    "\n",
    "#blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "#container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "#store = AzureBlobStore(container_client, zarr_store_path)\n",
    "#root = zarr.open(store, mode=\"w\")\n",
    "\n",
    "#y = np.arange(0, 18 * 2400)\n",
    "#x = np.arange(0, 36 * 2400)\n",
    "    #connection_string = os.environ[\"azure-storage-connection-string\"]\n",
    "#parts = azure_zarr_path.split(\"/\")\n",
    "\n",
    "#container_name = parts[2]\n",
    "#zarr_store_path = \"/\".join(parts[3:])\n",
    "\n",
    "# blob_service_client = BlobServiceClient.from_connection_string(\n",
    "#     connection_string\n",
    "# )\n",
    "#container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "#store = AzureBlobStore(connection_string,container_client, zarr_store_path)\n",
    "#root = zarr.open(store, mode=\"a\")\n",
    "\n",
    "\n",
    "# x_start, x_end = h * 2400, (h + 1) * 2400\n",
    "# y_start, y_end = v * 2400, (v + 1) * 2400\n",
    "        # with dask.config.set(pool=concurrent.futures.ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "# data = seasonal_snow_presence[['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']].to_array().values\n",
    "\n",
    "#'water_year':water_years,time_slice = slice(0, data.shape[0])\n",
    "                #seasonal_snow_presence.drop_vars('spatial_ref').chunk({'water_year':num_years,'y':2400,'x':2400}).to_zarr(store, region={'water_year':water_year_slice,'y':y_slice,'x':x_slice}, mode=\"r+\")\n",
    "\n",
    "\n",
    "# def check_environment():\n",
    "#     import sys\n",
    "#     import os\n",
    "#     result = {\n",
    "#         \"sys.path\": sys.path,\n",
    "#         \"current_dir\": os.getcwd(),\n",
    "#         \"list_dir\": os.listdir(),\n",
    "#         \"env_vars\": dict(os.environ),\n",
    "#     }\n",
    "#     try:\n",
    "#         import easysnowdata\n",
    "#         result[\"easysnowdata_version\"] = easysnowdata.__version__\n",
    "#     except ImportError as e:\n",
    "#         result[\"easysnowdata_error\"] = str(e)\n",
    "#     try:\n",
    "#         import modis_masking\n",
    "#         result[\"modis_masking_file\"] = modis_masking.__file__\n",
    "#     except ImportError as e:\n",
    "#         result[\"modis_masking_error\"] = str(e)\n",
    "#     return result\n",
    "\n",
    "# # Run this on all workers\n",
    "# environment_info = client.run(check_environment)\n",
    "\n",
    "# # Print the results\n",
    "# for worker, info in environment_info.items():\n",
    "#     print(f\"Worker {worker}:\")\n",
    "#     for key, value in info.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "#     print()\n",
    "\n",
    "\n",
    "# Set the Azure Blob Storage path for the zarr store\n",
    "#container_name = \"snowmelt\"\n",
    "#zarr_store_path = \"modis_mask/global_modis_snow_mask.zarr\"\n",
    "#azure_zarr_path = f\"azure://{container_name}/{zarr_store_path}\"\n",
    "\n",
    "\n",
    "\n",
    "# # Load progress\n",
    "# progress = load_progress()\n",
    "# processed_tiles = set(progress['processed'])\n",
    "# failed_tiles = set(progress['failed'])\n",
    "\n",
    "# # Load processed tiles from zarr\n",
    "# zarr_store = zarr.open(store, mode='r')\n",
    "# zarr_processed_tiles = set(zarr_store.attrs['processed_tiles'])\n",
    "\n",
    "\n",
    "\n",
    "# failed_tiles = []\n",
    "\n",
    "# def process_tile(tile, store):\n",
    "#     result = process_and_write_tile(tile, store)\n",
    "#     client.restart()  # Restart workers to clear memory\n",
    "#     return result\n",
    "\n",
    "# # First pass: process all tiles\n",
    "# for tile in tqdm.tqdm(tile_processing_list):\n",
    "\n",
    "#     try:\n",
    "#         result = process_tile(tile, store)\n",
    "#         print(f\"Tile {tile} processed and written\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing tile {tile}: {str(e)}\")\n",
    "#         print(f\"Traceback: {traceback.format_exc()}\")\n",
    "#         failed_tiles.append(tile)\n",
    "\n",
    "# # Second pass: retry failed tiles\n",
    "# max_retries = 3\n",
    "# retry_count = 0\n",
    "\n",
    "# while failed_tiles and retry_count < max_retries:\n",
    "#     retry_count += 1\n",
    "#     print(f\"Retry attempt {retry_count} for failed tiles\")\n",
    "#     still_failed = []\n",
    "    \n",
    "#     for tile in tqdm.tqdm(failed_tiles):\n",
    "#         try:\n",
    "#             result = process_tile(tile, store)\n",
    "#             print(f\"Tile {tile} processed and written on retry\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing tile {tile} on retry: {str(e)}\")\n",
    "#             print(f\"Traceback: {traceback.format_exc()}\")\n",
    "#             still_failed.append(tile)\n",
    "    \n",
    "#     failed_tiles = still_failed\n",
    "\n",
    "# if failed_tiles:\n",
    "#     print(\"The following tiles could not be processed after all retries:\")\n",
    "#     for tile in failed_tiles:\n",
    "#         print(f\"{tile}\")\n",
    "\n",
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_global_snowmelt_runoff_onset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
