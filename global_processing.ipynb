{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIS Snow Cover Data Processing with Dask and Azure Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install easysnowdata\n",
    "# !pip install coiled\n",
    "# !pip install geodatasets\n",
    "# !pip install adlfs\n",
    "# !pip install bottleneck\n",
    "#!pip install -U shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import zarr\n",
    "#from azure.storage.blob import BlobServiceClient\n",
    "import easysnowdata\n",
    "import modis_masking\n",
    "import coiled\n",
    "import tqdm\n",
    "import dask\n",
    "from dask.distributed import Client, wait\n",
    "import numcodecs\n",
    "import concurrent.futures\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "import geopandas as gpd\n",
    "import geodatasets\n",
    "import matplotlib.pyplot as plt\n",
    "import odc.stac\n",
    "import getpass\n",
    "import adlfs\n",
    "import json\n",
    "import pathlib\n",
    "odc.stac.configure_rio(cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WY_start = 2015\n",
    "WY_end = 2024\n",
    "\n",
    "# get token from https://github.com/egagli/azure_authentication/raw/main/sas_token.txt\n",
    "sas_token = pathlib.Path('sas_token.txt').read_text()\n",
    "\n",
    "store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snow_mask_v2/global_modis_snow_mask.zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and visualize MODIS Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_grid = gpd.read_file('zip+http://book.ecosens.org/wp-content/uploads/2016/06/modis_grid.zip!modis_sinusoidal_grid_world.shp')\n",
    "land = gpd.read_file(geodatasets.get_url('naturalearth land'))\n",
    "land_modis_crs = gpd.GeoSeries(land.union_all(), crs='EPSG:4326').to_crs(modis_grid.crs)\n",
    "modis_grid_land_idx = modis_grid.intersects(land_modis_crs.union_all())\n",
    "modis_grid_land_idx[600] = False\n",
    "modis_grid_land = modis_grid[modis_grid_land_idx]\n",
    "modis_grid_not_land = modis_grid[~modis_grid_land_idx]\n",
    "modis_grid_land_list = list(modis_grid_land.iterrows())\n",
    "tile_processing_list = [f'h{tile[\"h\"]}_v{tile[\"v\"]}' for _, tile in modis_grid_land_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15,15))\n",
    "land_modis_crs.plot(ax=ax, color='green')\n",
    "modis_grid_not_land.geometry.boundary.plot(ax=ax, color='gray', linewidth=0.5)\n",
    "modis_grid_land.geometry.boundary.plot(ax=ax, color='blue', linewidth=2)\n",
    "\n",
    "h_values = sorted(modis_grid['h'].unique())\n",
    "v_values = sorted(modis_grid['v'].unique(), reverse=True)\n",
    "\n",
    "h_coords = [modis_grid[modis_grid['h'] == h].geometry.centroid.x.mean() for h in h_values]\n",
    "v_coords = [modis_grid[modis_grid['v'] == v].geometry.centroid.y.mean() for v in v_values]\n",
    "\n",
    "ax.set_xticks(h_coords)\n",
    "ax.set_xticklabels([f'h{h}' for h in h_values])\n",
    "ax.set_yticks(v_coords)\n",
    "ax.set_yticklabels([f'v{v}' for v in v_values])\n",
    "\n",
    "ax.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "\n",
    "ax.set_title('MODIS grid system\\nland tiles in blue')\n",
    "ax.set_xlabel('Horizontal tile number')\n",
    "ax.set_ylabel('Vertical tile number')\n",
    "\n",
    "ax.set_xlim(modis_grid.total_bounds[0], modis_grid.total_bounds[2])\n",
    "ax.set_ylim(modis_grid.total_bounds[1], modis_grid.total_bounds[3])\n",
    "\n",
    "\n",
    "ax.set_title('MODIS grid system\\nland tiles in blue')\n",
    "f.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Processing Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_azure_zarr_store(store):\n",
    "\n",
    "    water_years = np.arange(WY_start, WY_end + 1)\n",
    "    num_years = len(water_years)\n",
    "\n",
    "\n",
    "    modis_snow_entire_extent_footprint = modis_masking.get_modis_MOD10A2_full_grid()\n",
    "    y = modis_snow_entire_extent_footprint.y.values\n",
    "    x = modis_snow_entire_extent_footprint.x.values\n",
    "\n",
    "\n",
    "    shape = (num_years, 18 * 2400, 36 * 2400)\n",
    "    chunks = (1, 2400, 2400)  \n",
    "\n",
    "    fill_value = np.iinfo(np.int16).min\n",
    "\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            'SAD_DOWY': (('water_year', 'y', 'x'), dask.array.full(shape, fill_value=fill_value, chunks=chunks, dtype=np.int16)),\n",
    "            'SDD_DOWY': (('water_year', 'y', 'x'), dask.array.full(shape, fill_value=fill_value, chunks=chunks, dtype=np.int16)),\n",
    "            'max_consec_snow_days': (('water_year', 'y', 'x'), dask.array.full(shape, fill_value=fill_value, chunks=chunks, dtype=np.int16)),\n",
    "        },\n",
    "        coords={\n",
    "            'water_year': water_years,\n",
    "            'y': y,\n",
    "            'x': x,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ds.water_year.attrs['description'] = (\"Water year. In northern hemisphere, water year starts on October 1st \"\n",
    "                                    \"and ends on September 30th. For the southern hemisphere, water year \"\n",
    "                                    \"starts on April 1st and ends on March 31st. e.g. in NH WY 2015 is \"\n",
    "                                    \"[2014-10-01,2015-09-30] and in SH WY 2015 is [2015-04-01,2016-03-31].\")\n",
    "    \n",
    "    ds.attrs['processed_tiles'] = []\n",
    "\n",
    "\n",
    "    encoding = {var: {'chunks': chunks, 'compressor': zarr.Blosc(cname='zstd', clevel=3, shuffle=zarr.Blosc.SHUFFLE)}\n",
    "                for var in ds.data_vars}\n",
    "    \n",
    "\n",
    "    ds.rio.write_crs(modis_snow_entire_extent_footprint.rio.crs, inplace=True)\n",
    "\n",
    "    # https://github.com/pydata/xarray/issues/6288#issuecomment-1230970216\n",
    "    for var in ds.data_vars:\n",
    "        ds[str(var)].attrs['grid_mapping'] = 'spatial_ref' \n",
    "    \n",
    "\n",
    "    ds.to_zarr(store, mode='w', encoding=encoding, compute=False, consolidated=True)\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(tile, store):\n",
    "    \n",
    "    h, v = (int(part[1:]) for part in tile.split('_'))\n",
    "\n",
    "    #odc.stac.configure_rio(cloud_defaults=True)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger('azure').setLevel(logging.WARNING)\n",
    "\n",
    "    logger.info(f\"Starting process for tile {tile}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        #logger.info(f\"Zarr store opened successfully\")\n",
    "\n",
    "        hemisphere = \"northern\" if v < 9 else \"southern\"\n",
    "        if hemisphere == \"southern\":\n",
    "            WY_end = 2023\n",
    "        else:\n",
    "            WY_end = 2024\n",
    "        \n",
    "        #logger.info(f\"Fetching MODIS data for tile {tile}\")\n",
    "        if hemisphere == \"northern\":\n",
    "            modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "                vertical_tile=v,\n",
    "                horizontal_tile=h,\n",
    "                start_date=f\"{WY_start-1}-10-01\",\n",
    "                end_date=f\"{WY_end}-09-30\",\n",
    "                #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "                chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "            ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "        else:\n",
    "            modis_snow_da = modis_masking.get_modis_MOD10A2_max_snow_extent(\n",
    "                vertical_tile=v,\n",
    "                horizontal_tile=h,\n",
    "                start_date=f\"{WY_start}-04-01\",\n",
    "                end_date=f\"{WY_end+1}-03-31\",\n",
    "                #chunks={\"time\": -1, \"y\": 600, \"x\": 600},\n",
    "                chunks={\"time\": 1, \"y\": 2400, \"x\": 2400},\n",
    "\n",
    "            ).chunk({\"time\": -1, \"y\": 600, \"x\": 600})\n",
    "\n",
    "        #logger.info(f\"Processing MODIS data for tile {tile}\")\n",
    "        modis_snow_da.coords[\"water_year\"] = (\n",
    "            \"time\",\n",
    "            pd.to_datetime(modis_snow_da.time).map(\n",
    "                lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere)\n",
    "            ),\n",
    "        )\n",
    "        modis_snow_da.coords[\"DOWY\"] = (\n",
    "            \"time\",\n",
    "            pd.to_datetime(modis_snow_da.time).map(\n",
    "                lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        modis_snow_da = modis_snow_da[\n",
    "            (modis_snow_da.water_year >= WY_start) & (modis_snow_da.water_year <= WY_end)\n",
    "        ]\n",
    "\n",
    "        #logger.info(f\"Applying binarize_with_cloud_filling for tile {tile}\")\n",
    "        effective_snow_da = modis_masking.binarize_with_cloud_filling(modis_snow_da)\n",
    "\n",
    "        #logger.info(f\"Calculating seasonal snow presence for tile {tile}\")\n",
    "        seasonal_snow_presence = effective_snow_da.groupby(\"water_year\").apply(\n",
    "            modis_masking.get_max_consec_snow_days_SAD_SDD_one_WY\n",
    "        )\n",
    "\n",
    "        #logger.info(f\"Writing results to zarr store for tile {tile}\")\n",
    "\n",
    "        num_years = len(seasonal_snow_presence.water_year)\n",
    "        water_year_slice = slice(0, num_years)\n",
    "        y_slice = slice(v * 2400, (v + 1) * 2400)\n",
    "        x_slice = slice(h * 2400, (h + 1) * 2400)\n",
    "\n",
    "        existing_ds = xr.open_zarr(store, consolidated=True)\n",
    "        y_coords = existing_ds.y[y_slice].values\n",
    "        x_coords = existing_ds.x[x_slice].values\n",
    "\n",
    "        if np.allclose(y_coords, seasonal_snow_presence.y.values, atol=0.1) or np.allclose(x_coords, seasonal_snow_presence.x.values, atol=0.1):\n",
    "            seasonal_snow_presence = seasonal_snow_presence.assign_coords(y=y_coords, x=x_coords)\n",
    "        else:\n",
    "            logger.error(f\"y or x coordinates do not match for tile {tile}\")\n",
    "            raise ValueError(f\"y or x coordinates do not match for tile {tile}\")\n",
    "\n",
    "        seasonal_snow_presence.drop_vars('spatial_ref').chunk({'water_year':1,'y':2400,'x':2400}).to_zarr(store, region=\"auto\", mode=\"r+\", consolidated=True)\n",
    "        #logger.info(f\"Tile {tile} processed and written successfully\")\n",
    "        #existing_ds.attrs['processed_tiles'].append(tile)\n",
    "        #logger.info(f\"Tile {tile} processed and written, added to processed_tiles list\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"(PT) Error processing tile {tile}: {str(e)}\")\n",
    "        logger.error(f\"(PT) Traceback: {traceback.format_exc()}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_fresh = False\n",
    "\n",
    "if start_fresh:\n",
    "    zarr_store_ds = create_azure_zarr_store(store)\n",
    "    zarr_store_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Dask Cluster with Coiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = coiled.Cluster(idle_timeout=\"15 minutes\",\n",
    "#                          n_workers=[20,100],\n",
    "#                          worker_memory=\"8 GiB\",\n",
    "#                          #worker_options={\"nthreads\": 8}, \n",
    "#                          spot_policy=\"spot\", \n",
    "#                          environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "#                          workspace=\"azure\",\n",
    "#                          )\n",
    "# #cluster.adapt(minimum=10, maximum=100)\n",
    "# client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-07 19:17:39,532 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,534 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,537 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,540 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,543 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,546 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,549 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,552 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,555 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,561 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,569 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,575 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,580 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,585 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,590 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,598 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,603 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,634 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,676 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,679 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,682 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,686 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,688 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,691 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,694 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,697 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,702 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,705 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,710 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,716 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,721 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,729 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,734 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,741 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,744 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,807 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,833 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,849 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,892 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,928 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:39,973 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,007 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,033 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,072 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,104 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,175 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,217 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,291 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,315 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,399 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,424 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,471 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,517 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,563 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,590 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,619 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,702 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,737 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,744 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,747 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,750 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,753 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,757 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,760 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,765 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,769 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,775 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,781 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,786 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,791 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,796 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,801 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,803 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,828 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,872 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,887 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,936 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,942 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:40,985 - distributed.nanny - WARNING - Restarting worker\n",
      "2024-10-07 19:17:41,017 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client.cluster.scale(100)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odc.stac.configure_rio(cloud_defaults=True, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process MODIS Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_tiles = []\n",
    "\n",
    "processed_tiles_list_initial = zarr.open(store).attrs['processed_tiles']\n",
    "for tile in tqdm.tqdm(tile_processing_list):\n",
    "    \n",
    "    if tile in processed_tiles_list_initial:\n",
    "        print(f\"Tile {tile} already processed, skipping\")\n",
    "        continue\n",
    "        \n",
    "    result = process_tile(tile, store)\n",
    "\n",
    "    if result == True:\n",
    "\n",
    "        with zarr.open(store) as zarr_store:\n",
    "            processed_tile_list = zarr_store.attrs['processed_tiles']\n",
    "            processed_tile_list.append(tile)\n",
    "            zarr_store.attrs['processed_tiles'] = processed_tile_list\n",
    "\n",
    "        print(f\"Tile {tile} SUCCESS, added to processed_list attribute\")\n",
    "\n",
    "        client.restart(wait_for_workers=True)\n",
    "\n",
    "    else:\n",
    "        print(f\"Tile {tile} FAIL, adding to failed list\")\n",
    "        failed_tiles.append(tile)\n",
    "\n",
    "\n",
    "if failed_tiles:\n",
    "    print(\"Run this cell again. The following tiles could not be processed:\")\n",
    "    for tile in failed_tiles:\n",
    "        print(tile)\n",
    "else:\n",
    "    print(\"Now consolidating metadata...\")\n",
    "    zarr.consolidate_metadata(store)\n",
    "    print(\"All tiles processed successfully!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other approaches (code graveyard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### serverless approach (this got close, couldn't push it across finish line though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.coiled.io/user_guide/functions.html\n",
    "# inspired by: https://github.com/earth-mover/serverless-datacube-demo/blob/main/src/lib.py\n",
    "\n",
    "# maybe another option: https://xarray.dev/blog/cubed-xarray\n",
    "# @coiled.function(\n",
    "#     n_workers=50,\n",
    "#     cpu=4,\n",
    "#     #threads_per_worker=8,\n",
    "#     memory=\"16GiB\",\n",
    "#     spot_policy=\"spot\",\n",
    "#     region=\"westeurope\",\n",
    "#     environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "#     keepalive=\"5m\",\n",
    "#     workspace=\"azure\"\n",
    "# )\n",
    "# # def process_chunks(tile_list, store):\n",
    "# #     odc.stac.configure_rio(cloud_defaults=True)\n",
    "# #     results = []\n",
    "# #     for _, tile in tile_list:\n",
    "# #         h = tile['h']\n",
    "# #         v = tile['v']\n",
    "# #         result = process_and_write_tile(h, v, store, serverless=False)\n",
    "# #         results.append(result)\n",
    "# #     return results\n",
    "# def process_chunk(tile, store):\n",
    "#     odc.stac.configure_rio(cloud_defaults=True)\n",
    "#     #with dask.config.set(pool=concurrent.futures.ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#     process = process_and_write_tile(tile, store, serverless=False)\n",
    "#     return process\n",
    "\n",
    "\n",
    "# def spawn_coiled_jobs(\n",
    "#     modis_grid_land_list, store):\n",
    "#     h_list = [tile['h'] for _, tile in modis_grid_land_list]\n",
    "#     v_list = [tile['v'] for _, tile in modis_grid_land_list]\n",
    "#     results = list(\n",
    "#         tqdm.tqdm(\n",
    "#             process_chunk.map(\n",
    "#                 h_list, \n",
    "#                 v_list,\n",
    "#                 store=store,\n",
    "#                 retries=5\n",
    "#             ),\n",
    "#             total=len(h_list),\n",
    "#             desc=\"Jobs Completed\",\n",
    "#         )\n",
    "#     )\n",
    "#     return results\n",
    "\n",
    "# # def spawn_coiled_jobs(modis_grid_land_list, store, batch_size=10):\n",
    "# #     batches = [modis_grid_land_list[i:i+batch_size] for i in range(0, len(modis_grid_land_list), batch_size)]\n",
    "# #     results = list(\n",
    "# #         tqdm.tqdm(\n",
    "# #             process_chunks.map(\n",
    "# #                 batches,\n",
    "# #                 store=store,\n",
    "# #                 retries=5\n",
    "# #             ),\n",
    "# #             total=len(batches),\n",
    "# #             desc=\"Batch completed\",\n",
    "# #         )\n",
    "# #     )\n",
    "# #     return [item for sublist in results for item in sublist]\n",
    "\n",
    "# #results = spawn_coiled_jobs(modis_grid_land_list, store)\n",
    "# #results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#futures = []\n",
    "# # for _, tile in tqdm.tqdm(modis_grid_land_list):\n",
    "# #     h = tile['h']\n",
    "# #     v = tile['v']\n",
    "# #     try:\n",
    "# #         process_and_write_tile(h, v, store)\n",
    "# #         print(f\"Tile h{h}_v{v} processed and written\")\n",
    "# #     except Exception as e:\n",
    "# #         print(f\"Error processing tile h{h}_v{v}: {str(e)}\")\n",
    "# #         print(f\"Traceback: {traceback.format_exc()}\")\n",
    "# #         # maybe append to a list of all tiles that need to be rerun\n",
    "# #     #future = client.submit(process_and_write_tile, h, v, store)\n",
    "# #     #futures.append(future)\n",
    "# # #results = wait(futures)\n",
    "# # \n",
    "# # for future in futures:\n",
    "#     try:\n",
    "#         result = future.result()\n",
    "#         print(result)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Task failed: {str(e)}\")\n",
    "#         print(f\"Traceback: {future.traceback()}\")\n",
    "# \n",
    "# \n",
    "# # client.close()\n",
    "# cluster.close()\n",
    "# \n",
    "#     #seasonal_snow_presence.drop_vars('spatial_ref').chunk({'water_year':1,'y':2400,'x':2400}).to_zarr(store, region={'water_year':water_year_slice,'y':y_slice,'x':x_slice}, mode=\"r+\")\n",
    "\n",
    "# if serverless:\n",
    "#     print(f'running serverless mode, using threadpoolexecutor...')\n",
    "#     with dask.config.set(pool=concurrent.futures.ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#         for var in ['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']:\n",
    "#             data = seasonal_snow_presence[var].values\n",
    "#             root[var][:,y_start:y_end,x_start:x_end] = data\n",
    "# else:\n",
    "#     for var in ['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']:\n",
    "#         data = seasonal_snow_presence[var].values\n",
    "#         root[var][:,y_start:y_end,x_start:x_end] = data\n",
    "\n",
    "# root[:, time_slice, y_slice, x_slice] = data\n",
    "\n",
    "\n",
    "    #root[var][time_slice, y_slice, x_slice] = data\n",
    "\n",
    "# if data.shape[0] == 9 and data.shape[1] == 2400 and data.shape[2] == 2400:\n",
    "#    print(f'transpose necessary h{h}_v{v}')\n",
    "#    data = np.transpose(data, (1, 2, 0))\n",
    "\n",
    "\n",
    "# store.flush()\n",
    "\n",
    "\n",
    "\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:  # Adjust number as needed\n",
    "#     futures = [executor.submit(process_and_write_tile, h, v, azure_zarr_path) \n",
    "#                for h, v in modis_grid_land_list]\n",
    "\n",
    "# def process_batch(batch):\n",
    "#     results = []\n",
    "#     for h, v in batch:\n",
    "#         results.append(process_and_write_tile(h, v, azure_zarr_path))\n",
    "#     return results\n",
    "\n",
    "# batch_size = 10  # Adjust based on your workload\n",
    "# batches = [modis_grid_land_list[i:i+batch_size] for i in range(0, len(modis_grid_land_list), batch_size)]\n",
    "# futures = client.map(process_batch, batches)\n",
    "\n",
    "\n",
    "\n",
    "# def create_azure_zarr_store(connection_string, container_name, zarr_store_path):\n",
    "#     blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "#     container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "#     class AzureBlobStore(zarr.ABSStore):\n",
    "#         def __init__(self, container_client, prefix):\n",
    "#             self.container_client = container_client\n",
    "#             self.prefix = prefix\n",
    "#             self.client = container_client  # Add this line\n",
    "\n",
    "#         def __getitem__(self, key):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             return blob_client.download_blob().readall()\n",
    "\n",
    "#         def __setitem__(self, key, value):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             blob_client.upload_blob(value, overwrite=True)\n",
    "\n",
    "#         def __contains__(self, key):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             return blob_client.exists()\n",
    "\n",
    "#         def __delitem__(self, key):\n",
    "#             blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#             blob_client.delete_blob()\n",
    "\n",
    "#         def rmdir(self, path):\n",
    "#             dir_path = self.prefix\n",
    "#             if path:\n",
    "#                 dir_path += \"/\" + path\n",
    "#             dir_path += \"/\"\n",
    "#             blobs_to_delete = self.container_client.list_blobs(\n",
    "#                 name_starts_with=dir_path\n",
    "#             )\n",
    "#             for blob in blobs_to_delete:\n",
    "#                 self.container_client.delete_blob(blob)\n",
    "\n",
    "#     store = AzureBlobStore(container_client, zarr_store_path)\n",
    "\n",
    "#     root = zarr.open(store, mode=\"w\")\n",
    "#     # root.create_dataset('SAD_DOWY', shape=(36 * 2400, 18 * 2400, len(range(WY_start, WY_end + 1))), chunks=(2400, 2400, 1), dtype='i2')\n",
    "#     # root.create_dataset('SDD_DOWY', shape=(36 * 2400, 18 * 2400, len(range(WY_start, WY_end + 1))), chunks=(2400, 2400, 1), dtype='i2')\n",
    "#     # root.create_dataset('max_consec_snow_days', shape=(36 * 2400, 18 * 2400, len(range(WY_start, WY_end + 1))), chunks=(2400, 2400, 1), dtype='i2')\n",
    "#     water_years = list(range(WY_start, WY_end + 1))\n",
    "\n",
    "#     num_years = len(water_years)\n",
    "\n",
    "#     compressor = numcodecs.Blosc(\n",
    "#         cname=\"zstd\", clevel=3, shuffle=numcodecs.Blosc.SHUFFLE\n",
    "#     )\n",
    "\n",
    "#     # Create datasets\n",
    "#     for var in ['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']:\n",
    "#         dataset = root.create_dataset(\n",
    "#             var,\n",
    "#             shape=(num_years, 18 * 2400, 36 * 2400),\n",
    "#             chunks=(1, 2400, 2400),\n",
    "#             dtype=\"i2\",\n",
    "#             compressor=compressor,\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "#         # Add dimension names as attributes\n",
    "\n",
    "#     #root.create_dataset(\"water_year\", data=water_years, shape=(num_years,), dtype=\"i2\")\n",
    "#     # root[\"time\"].attrs[\n",
    "#     #     \"description\"\n",
    "#     # ] = \"Water year. In northern hemisphere, water year starts on October 1st and ends on September 30th. For the southern hemisphere, water year starts on April 1st and ends on March 31st. For example, in the northern hemisphere water year 2015 starts on October 1st, 2014 and ends on September 30th, 2015, and in the southern hemisphere water year 2015 starts on April 1st, 2015 and ends on March 31st, 2016.\"\n",
    "\n",
    "\n",
    "#     return f\"azure://{container_name}/{zarr_store_path}\"\n",
    "\n",
    "# from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# class AzureBlobStore(zarr.ABSStore):\n",
    "#     def __init__(self, container_client, prefix):\n",
    "#         self.container_client = container_client\n",
    "#         self.prefix = prefix\n",
    "#         self.client = container_client  # Add this line\n",
    "\n",
    "#     def __getitem__(self, key):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         return blob_client.download_blob().readall()\n",
    "\n",
    "#     def __setitem__(self, key, value):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         blob_client.upload_blob(value, overwrite=True)\n",
    "\n",
    "#     def __contains__(self, key):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         return blob_client.exists()\n",
    "\n",
    "#     def __delitem__(self, key):\n",
    "#         blob_client = self.container_client.get_blob_client(f\"{self.prefix}/{key}\")\n",
    "#         blob_client.delete_blob()\n",
    "\n",
    "#     def rmdir(self, path):\n",
    "#         dir_path = self.prefix\n",
    "#         if path:\n",
    "#             dir_path += \"/\" + path\n",
    "#         dir_path += \"/\"\n",
    "#         blobs_to_delete = self.container_client.list_blobs(\n",
    "#             name_starts_with=dir_path\n",
    "#         )\n",
    "#         for blob in blobs_to_delete:\n",
    "#             self.container_client.delete_blob(blob)\n",
    "\n",
    "\n",
    "#blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "#container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "#store = AzureBlobStore(container_client, zarr_store_path)\n",
    "#root = zarr.open(store, mode=\"w\")\n",
    "\n",
    "#y = np.arange(0, 18 * 2400)\n",
    "#x = np.arange(0, 36 * 2400)\n",
    "    #connection_string = os.environ[\"azure-storage-connection-string\"]\n",
    "#parts = azure_zarr_path.split(\"/\")\n",
    "\n",
    "#container_name = parts[2]\n",
    "#zarr_store_path = \"/\".join(parts[3:])\n",
    "\n",
    "# blob_service_client = BlobServiceClient.from_connection_string(\n",
    "#     connection_string\n",
    "# )\n",
    "#container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "#store = AzureBlobStore(connection_string,container_client, zarr_store_path)\n",
    "#root = zarr.open(store, mode=\"a\")\n",
    "\n",
    "\n",
    "# x_start, x_end = h * 2400, (h + 1) * 2400\n",
    "# y_start, y_end = v * 2400, (v + 1) * 2400\n",
    "        # with dask.config.set(pool=concurrent.futures.ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "# data = seasonal_snow_presence[['SAD_DOWY', 'SDD_DOWY', 'max_consec_snow_days']].to_array().values\n",
    "\n",
    "#'water_year':water_years,time_slice = slice(0, data.shape[0])\n",
    "                #seasonal_snow_presence.drop_vars('spatial_ref').chunk({'water_year':num_years,'y':2400,'x':2400}).to_zarr(store, region={'water_year':water_year_slice,'y':y_slice,'x':x_slice}, mode=\"r+\")\n",
    "\n",
    "\n",
    "# def check_environment():\n",
    "#     import sys\n",
    "#     import os\n",
    "#     result = {\n",
    "#         \"sys.path\": sys.path,\n",
    "#         \"current_dir\": os.getcwd(),\n",
    "#         \"list_dir\": os.listdir(),\n",
    "#         \"env_vars\": dict(os.environ),\n",
    "#     }\n",
    "#     try:\n",
    "#         import easysnowdata\n",
    "#         result[\"easysnowdata_version\"] = easysnowdata.__version__\n",
    "#     except ImportError as e:\n",
    "#         result[\"easysnowdata_error\"] = str(e)\n",
    "#     try:\n",
    "#         import modis_masking\n",
    "#         result[\"modis_masking_file\"] = modis_masking.__file__\n",
    "#     except ImportError as e:\n",
    "#         result[\"modis_masking_error\"] = str(e)\n",
    "#     return result\n",
    "\n",
    "# # Run this on all workers\n",
    "# environment_info = client.run(check_environment)\n",
    "\n",
    "# # Print the results\n",
    "# for worker, info in environment_info.items():\n",
    "#     print(f\"Worker {worker}:\")\n",
    "#     for key, value in info.items():\n",
    "#         print(f\"  {key}: {value}\")\n",
    "#     print()\n",
    "\n",
    "\n",
    "# Set the Azure Blob Storage path for the zarr store\n",
    "#container_name = \"snowmelt\"\n",
    "#zarr_store_path = \"modis_mask/global_modis_snow_mask.zarr\"\n",
    "#azure_zarr_path = f\"azure://{container_name}/{zarr_store_path}\"\n",
    "\n",
    "\n",
    "\n",
    "# # Load progress\n",
    "# progress = load_progress()\n",
    "# processed_tiles = set(progress['processed'])\n",
    "# failed_tiles = set(progress['failed'])\n",
    "\n",
    "# # Load processed tiles from zarr\n",
    "# zarr_store = zarr.open(store, mode='r')\n",
    "# zarr_processed_tiles = set(zarr_store.attrs['processed_tiles'])\n",
    "\n",
    "\n",
    "\n",
    "# failed_tiles = []\n",
    "\n",
    "# def process_tile(tile, store):\n",
    "#     result = process_and_write_tile(tile, store)\n",
    "#     client.restart()  # Restart workers to clear memory\n",
    "#     return result\n",
    "\n",
    "# # First pass: process all tiles\n",
    "# for tile in tqdm.tqdm(tile_processing_list):\n",
    "\n",
    "#     try:\n",
    "#         result = process_tile(tile, store)\n",
    "#         print(f\"Tile {tile} processed and written\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing tile {tile}: {str(e)}\")\n",
    "#         print(f\"Traceback: {traceback.format_exc()}\")\n",
    "#         failed_tiles.append(tile)\n",
    "\n",
    "# # Second pass: retry failed tiles\n",
    "# max_retries = 3\n",
    "# retry_count = 0\n",
    "\n",
    "# while failed_tiles and retry_count < max_retries:\n",
    "#     retry_count += 1\n",
    "#     print(f\"Retry attempt {retry_count} for failed tiles\")\n",
    "#     still_failed = []\n",
    "    \n",
    "#     for tile in tqdm.tqdm(failed_tiles):\n",
    "#         try:\n",
    "#             result = process_tile(tile, store)\n",
    "#             print(f\"Tile {tile} processed and written on retry\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing tile {tile} on retry: {str(e)}\")\n",
    "#             print(f\"Traceback: {traceback.format_exc()}\")\n",
    "#             still_failed.append(tile)\n",
    "    \n",
    "#     failed_tiles = still_failed\n",
    "\n",
    "# if failed_tiles:\n",
    "#     print(\"The following tiles could not be processed after all retries:\")\n",
    "#     for tile in failed_tiles:\n",
    "#         print(f\"{tile}\")\n",
    "\n",
    "# client.close()\n",
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
